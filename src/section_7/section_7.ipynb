{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "60. 単語ベクトルの読み込みと表示\n",
    "\n",
    "Google Newsデータセット（約1,000億単語）での[学習済み単語ベクトル](https://drive.google.com/file/d/0B7XkCwpI5KDYNlNUTTlSS21pQmM/edit?usp=sharing)（300万単語・フレーズ，300次元）をダウンロードし，”United States”の単語ベクトルを表示せよ．ただし，”United States”は内部的には”United_States”と表現されていることに注意せよ．"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models import KeyedVectors\n",
    "import numpy as np\n",
    "\n",
    "model = KeyedVectors.load_word2vec_format('GoogleNews-vectors-negative300.bin.gz', binary=True)\n",
    "\n",
    "a = model[\"United_States\"]\n",
    "\n",
    "a.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "61. 単語の類似度\n",
    "\n",
    "“United States”と”U.S.”のコサイン類似度を計算せよ．"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "b = model[\"U.S.\"]\n",
    "cos_sim = np.dot(a, b) / (np.linalg.norm(a) * np.linalg.norm(b)) \n",
    "cos_sim"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "62. 類似度の高い単語10件\n",
    "\n",
    "“United States”とコサイン類似度が高い10語と，その類似度を出力せよ．"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.most_similar(\"Hello\",topn=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "63. 加法構成性によるアナロジー\n",
    "\n",
    "“Spain”の単語ベクトルから”Madrid”のベクトルを引き，”Athens”のベクトルを足したベクトルを計算し，そのベクトルと類似度の高い10語とその類似度を出力せよ．"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spain = model[\"Spain\"]\n",
    "madrid = model[\"Madrid\"]\n",
    "athens = model[\"Athens\"]\n",
    "res = spain - madrid + athens\n",
    "model.most_similar(res,topn=10)\n",
    "model.most_similar(positive=['Spain','Athens'], negative=['Madrid'],topn=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "64. アナロジーデータでの実験\n",
    "\n",
    "単語アナロジーの[評価データ](http://download.tensorflow.org/data/questions-words.txt)をダウンロードし，vec(2列目の単語) - vec(1列目の単語) + vec(3列目の単語)を計算し，そのベクトルと類似度が最も高い単語と，その類似度を求めよ．求めた単語と類似度は，各事例の末尾に追記せよ．"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "with open('questions-words.txt',\"r\") as reads:\n",
    "    sqs = [s.rstrip() for s in reads.readlines()]\n",
    "with open('sep.txt',\"w\") as writes:\n",
    "    length = len(sqs)\n",
    "    for i,col in enumerate(sqs):\n",
    "        q = col.split(' ')\n",
    "        if len(q) == 4:\n",
    "            best = model.most_similar(positive=[q[1],q[2]], negative=[q[0]])[0]\n",
    "            q += [best[0], str(best[1])]\n",
    "            output = ' '.join(q)+'\\n'\n",
    "        else:\n",
    "            output = col + '\\n'\n",
    "        writes.write(output)\n",
    "        os.system('clear')\n",
    "        reach = i*100/20000\n",
    "        if reach == round(reach):\n",
    "            print(str(reach)+'%')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "65. アナロジータスクでの正解率\n",
    "\n",
    "64の実行結果を用い，意味的アナロジー（semantic analogy）と文法的アナロジー（syntactic analogy）の正解率を測定せよ．"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('sep.txt',\"r\") as f:\n",
    "    semantic, syntactic = [], []\n",
    "    for line in f:\n",
    "        if line.startswith(\": gram\"):\n",
    "            ctg =  \"syntactic\"\n",
    "        elif line.startswith(\":\"):\n",
    "            ctg =  \"semantic\"\n",
    "        else:\n",
    "            if ctg == \"semantic\":\n",
    "                semantic.append(line.rstrip(\"\\n\").split())\n",
    "            if ctg == \"syntactic\":\n",
    "                syntactic.append(line.rstrip(\"\\n\").split())\n",
    "\n",
    "def calc(data):\n",
    "  \"\"\"data: List[List[str]{5}]\"\"\"\n",
    "  ndarray = np.array(data)\n",
    "  ans, prd = ndarray[:, 3:5].T\n",
    "  return np.mean(ans==prd)\n",
    "\n",
    "print(\"semantic: \", calc(semantic))\n",
    "print(\"syntactic: \", calc(syntactic))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "66. WordSimilarity-353での評価\n",
    "\n",
    "[The WordSimilarity-353 Test Collection](http://www.gabrilovich.com/resources/data/wordsim353/wordsim353.html)の評価データをダウンロードし，単語ベクトルにより計算される類似度のランキングと，人間の類似度判定のランキングの間のスピアマン相関係数を計算せよ．"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.stats import spearmanr\n",
    "\n",
    "sim_rank, human_rank = [], []\n",
    "with open('wordsim353/combined.csv',\"r\") as f:\n",
    "    for line in list(f)[1:]:\n",
    "        word1,word2,human = line.replace('\\n','').split(\",\")\n",
    "        if word1 in model and word2 in model:\n",
    "            x = model.similarity(word1,word2)\n",
    "            sim_rank.append(x)\n",
    "            human_rank.append(human)\n",
    "\n",
    "correlation, pvalue = spearmanr(sim_rank, human_rank)\n",
    "print(correlation, pvalue)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "67. k-meansクラスタリング\n",
    "\n",
    "国名に関する単語ベクトルを抽出し，k-meansクラスタリングをクラスタ数k=5として実行せよ．"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "country = {}\n",
    "show_list = []\n",
    "with open('questions-words.txt',\"r\") as f:\n",
    "    for col in f:\n",
    "        if col.startswith(\": currency\"):\n",
    "            break\n",
    "        elif col.startswith(\":\"):\n",
    "            continue\n",
    "        else:\n",
    "            _,w,_,c=col.replace(\"\\n\",\"\").split()\n",
    "            if c not in country and c in model:\n",
    "                country[c] = model[c]\n",
    "                show_list.append(model[c])\n",
    "            if w not in country and w in model:\n",
    "                country[w] = model[w]\n",
    "                show_list.append(model[w])\n",
    "\n",
    "from sklearn.cluster import KMeans\n",
    "\n",
    "kmeans_model_predict = KMeans(n_clusters=5, random_state=10, n_init='auto').fit_predict(list(country.values()))\n",
    "print([list(country.keys())[i] for i, p in enumerate(kmeans_model_predict) if p == 1])\n",
    "\n",
    "from collections import Counter\n",
    "c = Counter(kmeans_model_predict)\n",
    "print(c)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "68. Ward法によるクラスタリング\n",
    "\n",
    "国名に関する単語ベクトルに対し，Ward法による階層型クラスタリングを実行せよ．さらに，クラスタリング結果をデンドログラムとして可視化せよ．"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.cluster.hierarchy import linkage,dendrogram\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.figure(figsize=(36, 12))\n",
    "Z = linkage(list(country.values()), method='ward')\n",
    "dendrogram(Z, labels=list(country.keys()))\n",
    "plt.xticks(fontsize=14)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "69. t-SNE による可視化\n",
    "\n",
    "ベクトル空間上の国名に関する単語ベクトルをt-SNEで可視化せよ．"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.manifold import TSNE\n",
    "tsne = TSNE(n_components=2, random_state=0)\n",
    "X = tsne.fit_transform(np.array(show_list))\n",
    "\n",
    "plt.scatter(X[:,0], X[:,1])\n",
    "plt.colorbar()\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
