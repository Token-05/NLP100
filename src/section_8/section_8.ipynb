{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[46], line 8\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m\n\u001b[1;32m      6\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\n\u001b[0;32m----> 8\u001b[0m model \u001b[38;5;241m=\u001b[39m \u001b[43mKeyedVectors\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload_word2vec_format\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43m../section_7/GoogleNews-vectors-negative300.bin.gz\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbinary\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Documents/tuat/study/B4/model/lib/python3.12/site-packages/gensim/models/keyedvectors.py:1719\u001b[0m, in \u001b[0;36mKeyedVectors.load_word2vec_format\u001b[0;34m(cls, fname, fvocab, binary, encoding, unicode_errors, limit, datatype, no_header)\u001b[0m\n\u001b[1;32m   1672\u001b[0m \u001b[38;5;129m@classmethod\u001b[39m\n\u001b[1;32m   1673\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mload_word2vec_format\u001b[39m(\n\u001b[1;32m   1674\u001b[0m         \u001b[38;5;28mcls\u001b[39m, fname, fvocab\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, binary\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m, encoding\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mutf8\u001b[39m\u001b[38;5;124m'\u001b[39m, unicode_errors\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mstrict\u001b[39m\u001b[38;5;124m'\u001b[39m,\n\u001b[1;32m   1675\u001b[0m         limit\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, datatype\u001b[38;5;241m=\u001b[39mREAL, no_header\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[1;32m   1676\u001b[0m     ):\n\u001b[1;32m   1677\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Load KeyedVectors from a file produced by the original C word2vec-tool format.\u001b[39;00m\n\u001b[1;32m   1678\u001b[0m \n\u001b[1;32m   1679\u001b[0m \u001b[38;5;124;03m    Warnings\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1717\u001b[0m \n\u001b[1;32m   1718\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m-> 1719\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_load_word2vec_format\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1720\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mcls\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfname\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfvocab\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfvocab\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbinary\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbinary\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mencoding\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mencoding\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43municode_errors\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43municode_errors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1721\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlimit\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlimit\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdatatype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdatatype\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mno_header\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mno_header\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1722\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Documents/tuat/study/B4/model/lib/python3.12/site-packages/gensim/models/keyedvectors.py:2065\u001b[0m, in \u001b[0;36m_load_word2vec_format\u001b[0;34m(cls, fname, fvocab, binary, encoding, unicode_errors, limit, datatype, no_header, binary_chunk_size)\u001b[0m\n\u001b[1;32m   2062\u001b[0m kv \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mcls\u001b[39m(vector_size, vocab_size, dtype\u001b[38;5;241m=\u001b[39mdatatype)\n\u001b[1;32m   2064\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m binary:\n\u001b[0;32m-> 2065\u001b[0m     \u001b[43m_word2vec_read_binary\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   2066\u001b[0m \u001b[43m        \u001b[49m\u001b[43mfin\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkv\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcounts\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvocab_size\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvector_size\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdatatype\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43municode_errors\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbinary_chunk_size\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mencoding\u001b[49m\n\u001b[1;32m   2067\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   2068\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   2069\u001b[0m     _word2vec_read_text(fin, kv, counts, vocab_size, vector_size, datatype, unicode_errors, encoding)\n",
      "File \u001b[0;32m~/Documents/tuat/study/B4/model/lib/python3.12/site-packages/gensim/models/keyedvectors.py:1958\u001b[0m, in \u001b[0;36m_word2vec_read_binary\u001b[0;34m(fin, kv, counts, vocab_size, vector_size, datatype, unicode_errors, binary_chunk_size, encoding)\u001b[0m\n\u001b[1;32m   1955\u001b[0m tot_processed_words \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[1;32m   1957\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m tot_processed_words \u001b[38;5;241m<\u001b[39m vocab_size:\n\u001b[0;32m-> 1958\u001b[0m     new_chunk \u001b[38;5;241m=\u001b[39m \u001b[43mfin\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbinary_chunk_size\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1959\u001b[0m     chunk \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m new_chunk\n\u001b[1;32m   1960\u001b[0m     processed_words, chunk \u001b[38;5;241m=\u001b[39m _add_bytes_to_kv(\n\u001b[1;32m   1961\u001b[0m         kv, counts, chunk, vocab_size, vector_size, datatype, unicode_errors, encoding)\n",
      "File \u001b[0;32m/opt/homebrew/Cellar/python@3.12/3.12.3/Frameworks/Python.framework/Versions/3.12/lib/python3.12/gzip.py:324\u001b[0m, in \u001b[0;36mGzipFile.read\u001b[0;34m(self, size)\u001b[0m\n\u001b[1;32m    322\u001b[0m     \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01merrno\u001b[39;00m\n\u001b[1;32m    323\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mOSError\u001b[39;00m(errno\u001b[38;5;241m.\u001b[39mEBADF, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mread() on write-only GzipFile object\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m--> 324\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_buffer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread\u001b[49m\u001b[43m(\u001b[49m\u001b[43msize\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/homebrew/Cellar/python@3.12/3.12.3/Frameworks/Python.framework/Versions/3.12/lib/python3.12/_compression.py:68\u001b[0m, in \u001b[0;36mDecompressReader.readinto\u001b[0;34m(self, b)\u001b[0m\n\u001b[1;32m     66\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mreadinto\u001b[39m(\u001b[38;5;28mself\u001b[39m, b):\n\u001b[1;32m     67\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mmemoryview\u001b[39m(b) \u001b[38;5;28;01mas\u001b[39;00m view, view\u001b[38;5;241m.\u001b[39mcast(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mB\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;28;01mas\u001b[39;00m byte_view:\n\u001b[0;32m---> 68\u001b[0m         data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mlen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mbyte_view\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     69\u001b[0m         byte_view[:\u001b[38;5;28mlen\u001b[39m(data)] \u001b[38;5;241m=\u001b[39m data\n\u001b[1;32m     70\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(data)\n",
      "File \u001b[0;32m/opt/homebrew/Cellar/python@3.12/3.12.3/Frameworks/Python.framework/Versions/3.12/lib/python3.12/gzip.py:535\u001b[0m, in \u001b[0;36m_GzipReader.read\u001b[0;34m(self, size)\u001b[0m\n\u001b[1;32m    533\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_decompressor\u001b[38;5;241m.\u001b[39mneeds_input:\n\u001b[1;32m    534\u001b[0m     buf \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_fp\u001b[38;5;241m.\u001b[39mread(READ_BUFFER_SIZE)\n\u001b[0;32m--> 535\u001b[0m     uncompress \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_decompressor\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdecompress\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbuf\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msize\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    536\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    537\u001b[0m     uncompress \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_decompressor\u001b[38;5;241m.\u001b[39mdecompress(\u001b[38;5;124mb\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\"\u001b[39m, size)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "from gensim.models import KeyedVectors\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "import tensorflow\n",
    "\n",
    "model = KeyedVectors.load_word2vec_format('../section_7/GoogleNews-vectors-negative300.bin.gz', binary=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "70. 単語ベクトルの和による特徴量\n",
    "\n",
    "XとYについて、訓練データ、検証データ、テストデータを作成"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# csvとして読み取り、namesにヘッダを追加\n",
    "df = pd.read_csv('../section_6/news+aggregator/newsCorpora.tsv', sep='\\t', header=0, names=['TITLE','URL','PUBLISHER','CATEGORY','STORY','HOSTNAME','TIMESTAMP'])\n",
    "\n",
    "valid_publishers = [\"Reuters\", \"Huffington Post\", \"Businessweek\", \"Contactmusic.com\", \"Daily Mail\"]\n",
    "df_publisher = df[df['PUBLISHER'].isin(valid_publishers)]\n",
    "df_shuffle = df_publisher.sample(frac=1)\n",
    "\n",
    "# 分割手続き\n",
    "length = len(df_shuffle)\n",
    "eighty = (length//100)*80\n",
    "ninety = (length//100)*90\n",
    "(train,valid,test) = (df_shuffle[:eighty],df_shuffle[eighty:ninety],df_shuffle[ninety:])\n",
    "\n",
    "# Xについて作成\n",
    "\n",
    "for key, value in {'X_train.txt':train,'X_test.txt':test,'X_valid.txt':valid}.items():\n",
    "    with open('data/'+key,'w') as f:\n",
    "        for text in value['TITLE']:\n",
    "            vec = []\n",
    "            for word in text.split():\n",
    "                if word in model:\n",
    "                    vec.append(model[word])\n",
    "            if not len(vec):\n",
    "                vec = np.zeros(300)\n",
    "            else:\n",
    "                vec = np.array(vec)\n",
    "                vec = vec.mean(axis=0)\n",
    "            vec = vec.astype(np.str_).tolist()\n",
    "            output = ' '.join(vec)+'\\n'\n",
    "            f.write(output)\n",
    "\n",
    "# Yについて作成\n",
    "\n",
    "sep_dict = {\"b\":0,\"t\":1,\"e\":2,\"m\":3}\n",
    "\n",
    "Y_train = [sep_dict[c] for c in train['CATEGORY'].to_numpy()]\n",
    "Y_valid = [sep_dict[c] for c in valid['CATEGORY'].to_numpy()]\n",
    "Y_test = [sep_dict[c] for c in test['CATEGORY'].to_numpy()]\n",
    "\n",
    "np.savetxt('data/Y_train.txt', Y_train, fmt='%d')\n",
    "np.savetxt('data/Y_valid.txt', Y_valid, fmt='%d')\n",
    "np.savetxt('data/Y_test.txt', Y_test, fmt='%d')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "71. 単層ニューラルネットワークによる予測\n",
    "\n",
    "パーセプトロンの順伝播での計算"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.4634, 0.4984, 0.0276, 0.0106]], dtype=torch.float64)\n",
      "tensor([[0.4634, 0.4984, 0.0276, 0.0106],\n",
      "        [0.0046, 0.9760, 0.0161, 0.0032],\n",
      "        [0.0311, 0.8073, 0.0771, 0.0845],\n",
      "        [0.5015, 0.4444, 0.0437, 0.0105]], dtype=torch.float64)\n"
     ]
    }
   ],
   "source": [
    "X = np.loadtxt(\"data/X_train.txt\", delimiter=' ') \n",
    "X = torch.tensor(X,dtype=torch.float64)\n",
    "\n",
    "g = torch.Generator()\n",
    "g.manual_seed(523)\n",
    "W = torch.randn(300,4, dtype=torch.float64)\n",
    "\n",
    "scores_1 = X[:1] @ W\n",
    "scores_4 = X[:4] @ W\n",
    "\n",
    "softmax = torch.nn.Softmax(dim=1)\n",
    "print(softmax(scores_1))\n",
    "print(softmax(scores_4))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "72. 損失と勾配の計算\n",
    "\n",
    "損失と勾配を計算せよ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(0.7692, dtype=torch.float64) tensor(2.0370, dtype=torch.float64)\n",
      "2.037038505908271\n"
     ]
    }
   ],
   "source": [
    "Y = np.loadtxt(\"data/Y_train.txt\", delimiter=' ')\n",
    "Y = torch.tensor(Y,dtype=torch.float64)\n",
    "\n",
    "loss = torch.nn.CrossEntropyLoss()\n",
    "\n",
    "loss_1 = loss(scores_1,Y[:1].type(torch.long))\n",
    "loss_4 = loss(scores_4,Y[:4].type(torch.long))\n",
    "\n",
    "print(loss_1,loss_4)\n",
    "\n",
    "ans = []\n",
    "for s,i in zip(softmax(scores_4),Y[:4].type(torch.long)):\n",
    "  ans.append(-np.log(s[i]))\n",
    "print (np.mean(ans))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "73. 確率的勾配降下法による学習\n",
    "\n",
    "確率的勾配降下法（SGD: Stochastic Gradient Descent）を用いて，行列Wを学習せよ．なお，学習は適当な基準で終了させればよい（例えば「100エポックで終了」など）"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = np.loadtxt(\"data/X_train.txt\", delimiter=' ') \n",
    "X = torch.tensor(X,dtype=torch.float64)\n",
    "\n",
    "Y = np.loadtxt(\"data/Y_train.txt\", delimiter=' ')\n",
    "Y = torch.tensor(Y,dtype=torch.long)\n",
    "\n",
    "W = torch.randn(300,4, dtype=torch.float64, requires_grad=True)\n",
    "\n",
    "loss = torch.nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.SGD([W], lr=0.1, momentum=0.9)\n",
    "\n",
    "for epoch in range(100):\n",
    "    losses = loss(X @ W,Y)\n",
    "    optimizer.zero_grad()\n",
    "    losses.backward()\n",
    "    optimizer.step()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "74. 正解率の計測\n",
    "\n",
    "問題73で求めた行列を用いて学習データおよび評価データの事例を分類したとき，その正解率をそれぞれ求めよ．"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.750093984962406\n",
      "0.7518796992481203\n"
     ]
    }
   ],
   "source": [
    "def acc(pred, label):\n",
    "    pred = torch.argmax(pred, axis=1).numpy()\n",
    "    label = label.data.numpy()\n",
    "    return (pred == label).mean()\n",
    "\n",
    "X_valid = np.loadtxt(\"data/X_valid.txt\", delimiter=' ') \n",
    "X_valid = torch.tensor(X_valid,dtype=torch.float64)\n",
    "\n",
    "Y_valid = np.loadtxt(\"data/Y_valid.txt\", delimiter=' ')\n",
    "Y_valid = torch.tensor(Y_valid,dtype=torch.long)\n",
    "\n",
    "print(acc(X @ W,Y))\n",
    "print(acc(X_valid @ W,Y_valid))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "75. 損失と正解率のプロット + 74.にデータの正規化を入れてまとめたもの\n",
    "\n",
    "問題73のコードを改変し，各エポックのパラメータ更新が完了するたびに，訓練データでの損失，正解率，検証データでの損失，正解率をグラフにプロットし，学習の進捗状況を確認できるようにせよ．"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Accuracy: 0.8809210526315789\n",
      "Validation Accuracy: 0.8413533834586466\n"
     ]
    }
   ],
   "source": [
    "from torch.utils.tensorboard import SummaryWriter\n",
    "\n",
    "# データの読み込みと正規化\n",
    "def load_and_normalize(file_path):\n",
    "    data = np.loadtxt(file_path, delimiter=' ')\n",
    "    mean = np.mean(data, axis=0)\n",
    "    std = np.std(data, axis=0)\n",
    "    return (data - mean) / std\n",
    "\n",
    "# 名前を少しわかりやすく\n",
    "X_train = load_and_normalize(\"data/X_train.txt\")\n",
    "X_train = torch.tensor(X_train, dtype=torch.float64)\n",
    "\n",
    "Y_train = np.loadtxt(\"data/Y_train.txt\", delimiter=' ')\n",
    "Y_train = torch.tensor(Y_train, dtype=torch.long)\n",
    "\n",
    "X_valid = load_and_normalize(\"data/X_valid.txt\")\n",
    "X_valid = torch.tensor(X_valid, dtype=torch.float64)\n",
    "\n",
    "Y_valid = np.loadtxt(\"data/Y_valid.txt\", delimiter=' ')\n",
    "Y_valid = torch.tensor(Y_valid, dtype=torch.long)\n",
    "\n",
    "W = torch.randn(300, 4, dtype=torch.float64, requires_grad=True)\n",
    "\n",
    "loss_fn = torch.nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.SGD([W], lr=0.01, momentum=0.9)\n",
    "writer = SummaryWriter(log_dir=\"log\")\n",
    "\n",
    "# 学習\n",
    "for epoch in range(2500):\n",
    "    scores = X_train @ W\n",
    "    loss = loss_fn(scores, Y_train)\n",
    "    \n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "    with torch.no_grad():\n",
    "        Y_pred = X_train @ W\n",
    "        loss = loss_fn(Y_pred, Y_train) \n",
    "        writer.add_scalar('Loss/train', loss, epoch)\n",
    "        writer.add_scalar('Accuracy/train', acc(Y_pred,Y_train), epoch)\n",
    "\n",
    "        Y_pred = X_valid @ W\n",
    "        loss = loss_fn(Y_pred, Y_valid)\n",
    "        writer.add_scalar('Loss/valid', loss, epoch)\n",
    "        writer.add_scalar('Accuracy/valid', acc(Y_pred,Y_valid), epoch)\n",
    "\n",
    "\n",
    "def acc(pred, label):\n",
    "    pred = torch.argmax(pred, axis=1).numpy()\n",
    "    label = label.numpy()\n",
    "    return (pred == label).mean()\n",
    "\n",
    "train_acc = acc(X_train @ W, Y_train)\n",
    "valid_acc = acc(X_valid @ W, Y_valid)\n",
    "\n",
    "print(f\"Training Accuracy: {train_acc}\")\n",
    "print(f\"Validation Accuracy: {valid_acc}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "76. チェックポイント\n",
    "\n",
    "問題75のコードを改変し，各エポックのパラメータ更新が完了するたびに，チェックポイント（学習途中のパラメータ（重み行列など）の値や最適化アルゴリズムの内部状態）をファイルに書き出せ．"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Accuracy: 0.6862781954887218\n",
      "Validation Accuracy: 0.6879699248120301\n"
     ]
    }
   ],
   "source": [
    "# データの読み込みと正規化\n",
    "def load_and_normalize(file_path):\n",
    "    data = np.loadtxt(file_path, delimiter=' ')\n",
    "    mean = np.mean(data, axis=0)\n",
    "    std = np.std(data, axis=0)\n",
    "    return (data - mean) / std\n",
    "\n",
    "# 名前を少しわかりやすく\n",
    "X_train = load_and_normalize(\"data/X_train.txt\")\n",
    "X_train = torch.tensor(X_train, dtype=torch.float64)\n",
    "\n",
    "Y_train = np.loadtxt(\"data/Y_train.txt\", delimiter=' ')\n",
    "Y_train = torch.tensor(Y_train, dtype=torch.long)\n",
    "\n",
    "X_valid = load_and_normalize(\"data/X_valid.txt\")\n",
    "X_valid = torch.tensor(X_valid, dtype=torch.float64)\n",
    "\n",
    "Y_valid = np.loadtxt(\"data/Y_valid.txt\", delimiter=' ')\n",
    "Y_valid = torch.tensor(Y_valid, dtype=torch.long)\n",
    "\n",
    "W = torch.randn(300, 4, dtype=torch.float64, requires_grad=True)\n",
    "\n",
    "loss_fn = torch.nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.SGD([W], lr=0.1, momentum=0.9)\n",
    "writer = SummaryWriter(log_dir=\"log\")\n",
    "\n",
    "for epoch in range(10):\n",
    "    scores = X_train @ W\n",
    "    loss = loss_fn(scores, Y_train)\n",
    "    \n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "    with torch.no_grad():\n",
    "        Y_pred = X_train @ W\n",
    "        loss = loss_fn(Y_pred, Y_train) \n",
    "        writer.add_scalar('Loss/train', loss, epoch)\n",
    "        writer.add_scalar('Accuracy/train', acc(Y_pred,Y_train), epoch)\n",
    "\n",
    "        Y_pred = X_valid @ W\n",
    "        loss = loss_fn(Y_pred, Y_valid)\n",
    "        writer.add_scalar('Loss/valid', loss, epoch)\n",
    "        writer.add_scalar('Accuracy/valid', acc(Y_pred,Y_valid), epoch)\n",
    "\n",
    "    checkpoint = {\n",
    "        'epoch': epoch,\n",
    "        'model_state_dict': W,\n",
    "        'optimizer_state_dict': optimizer.state_dict()\n",
    "    }\n",
    "    torch.save(checkpoint, f\"cp/epoch_{epoch}.pt\")\n",
    "\n",
    "def acc(pred, label):\n",
    "    pred = torch.argmax(pred, axis=1).numpy()\n",
    "    label = label.numpy()\n",
    "    return (pred == label).mean()\n",
    "\n",
    "train_acc = acc(X_train @ W, Y_train)\n",
    "valid_acc = acc(X_valid @ W, Y_valid)\n",
    "\n",
    "print(f\"Training Accuracy: {train_acc}\")\n",
    "print(f\"Validation Accuracy: {valid_acc}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "77. ミニバッチ化\n",
    "\n",
    "問題76のコードを改変し，B事例ごとに損失・勾配を計算し，行列Wの値を更新せよ（ミニバッチ化）．Bの値を1,2,4,8,…と変化させながら，1エポックの学習に要する時間を比較せよ．"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch size 1: 9.457581043243408 seconds\n",
      "Batch size 2: 9.055637121200562 seconds\n",
      "Batch size 4: 4.530675888061523 seconds\n",
      "Batch size 8: 2.3388307094573975 seconds\n",
      "Batch size 16: 1.2995638847351074 seconds\n",
      "Batch size 32: 0.6992628574371338 seconds\n",
      "Batch size 64: 0.3909733295440674 seconds\n",
      "Batch size 128: 0.23469328880310059 seconds\n",
      "Batch size 256: 0.151123046875 seconds\n",
      "Training Accuracy: 0.8953007518796993\n",
      "Validation Accuracy: 0.849624060150376\n",
      "Epoch time results: {1: 9.457581043243408, 2: 9.055637121200562, 4: 4.530675888061523, 8: 2.3388307094573975, 16: 1.2995638847351074, 32: 0.6992628574371338, 64: 0.3909733295440674, 128: 0.23469328880310059, 256: 0.151123046875}\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "\n",
    "# データの読み込みと正規化\n",
    "def load_and_normalize(file_path):\n",
    "    data = np.loadtxt(file_path, delimiter=' ')\n",
    "    mean = np.mean(data, axis=0)\n",
    "    std = np.std(data, axis=0)\n",
    "    return (data - mean) / std\n",
    "\n",
    "# 名前を少しわかりやすく\n",
    "X_train = load_and_normalize(\"data/X_train.txt\")\n",
    "X_train = torch.tensor(X_train, dtype=torch.float64)\n",
    "\n",
    "Y_train = np.loadtxt(\"data/Y_train.txt\", delimiter=' ')\n",
    "Y_train = torch.tensor(Y_train, dtype=torch.long)\n",
    "\n",
    "X_valid = load_and_normalize(\"data/X_valid.txt\")\n",
    "X_valid = torch.tensor(X_valid, dtype=torch.float64)\n",
    "\n",
    "Y_valid = np.loadtxt(\"data/Y_valid.txt\", delimiter=' ')\n",
    "Y_valid = torch.tensor(Y_valid, dtype=torch.long)\n",
    "\n",
    "loss_fn = torch.nn.CrossEntropyLoss()\n",
    "writer = SummaryWriter(log_dir=\"log\")\n",
    "\n",
    "def acc(pred, label):\n",
    "    pred = torch.argmax(pred, axis=1).numpy()\n",
    "    label = label.numpy()\n",
    "    return (pred == label).mean()\n",
    "\n",
    "batch_sizes = [1, 2, 4, 8, 16, 32, 64, 128, 256]\n",
    "epoch_time_results = {}\n",
    "\n",
    "for B in batch_sizes:\n",
    "    W = torch.randn(300, 4, dtype=torch.float64, requires_grad=True)\n",
    "    optimizer = torch.optim.SGD([W], lr=0.1, momentum=0.9)\n",
    "\n",
    "    start_time = time.time()\n",
    "    \n",
    "    for epoch in range(10):\n",
    "        permutation = torch.randperm(X_train.size()[0])\n",
    "        \n",
    "        for i in range(0, X_train.size()[0], B):\n",
    "            indices = permutation[i:i+B]\n",
    "            batch_X, batch_Y = X_train[indices], Y_train[indices]\n",
    "            \n",
    "            scores = batch_X @ W\n",
    "            loss = loss_fn(scores, batch_Y)\n",
    "            \n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "        with torch.no_grad():\n",
    "            Y_pred = X_train @ W\n",
    "            train_loss = loss_fn(Y_pred, Y_train) \n",
    "            writer.add_scalar(f'Loss/train_batch_{B}', train_loss, epoch)\n",
    "            writer.add_scalar(f'Accuracy/train_batch_{B}', acc(Y_pred,Y_train), epoch)\n",
    "\n",
    "            Y_pred = X_valid @ W\n",
    "            valid_loss = loss_fn(Y_pred, Y_valid)\n",
    "            writer.add_scalar(f'Loss/valid_batch_{B}', valid_loss, epoch)\n",
    "            writer.add_scalar(f'Accuracy/valid_batch_{B}', acc(Y_pred,Y_valid), epoch)\n",
    "\n",
    "    end_time = time.time()\n",
    "    epoch_time_results[B] = end_time - start_time\n",
    "    print(f\"Batch size {B}: {epoch_time_results[B]} seconds\")\n",
    "\n",
    "    checkpoint = {\n",
    "        'epoch': epoch,\n",
    "        'model_state_dict': W,\n",
    "        'optimizer_state_dict': optimizer.state_dict()\n",
    "    }\n",
    "    torch.save(checkpoint, f\"cp/batch_{B}_epoch_{epoch}.pt\")\n",
    "\n",
    "print(\"Training Accuracy:\", acc(X_train @ W, Y_train))\n",
    "print(\"Validation Accuracy:\", acc(X_valid @ W, Y_valid))\n",
    "print(\"Epoch time results:\", epoch_time_results)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "78. GPU上での学習\n",
    "\n",
    "問題77のコードを改変し，GPU上で学習を実行せよ．（パーセプトロンの仕組みがわかったので`torch.nn.Module`をオーバーライドしてみる）"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch size 1: 83.89398980140686 seconds\n",
      "Batch size 2: 41.44503307342529 seconds\n",
      "Batch size 4: 21.423542022705078 seconds\n",
      "Batch size 8: 10.67266297340393 seconds\n",
      "Batch size 16: 5.2827489376068115 seconds\n",
      "Batch size 32: 2.578295946121216 seconds\n",
      "Batch size 64: 1.4275047779083252 seconds\n",
      "Batch size 128: 0.7153580188751221 seconds\n",
      "Batch size 256: 0.37299609184265137 seconds\n",
      "Training Accuracy: 0.8883458646616541\n",
      "Validation Accuracy: 0.8473684210526315\n",
      "Epoch time results: {1: 83.89398980140686, 2: 41.44503307342529, 4: 21.423542022705078, 8: 10.67266297340393, 16: 5.2827489376068115, 32: 2.578295946121216, 64: 1.4275047779083252, 128: 0.7153580188751221, 256: 0.37299609184265137}\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "\n",
    "# データの読み込みと正規化\n",
    "def load_and_normalize(file_path):\n",
    "    data = np.loadtxt(file_path, delimiter=' ')\n",
    "    mean = np.mean(data, axis=0)\n",
    "    std = np.std(data, axis=0)\n",
    "    return (data - mean) / std\n",
    "\n",
    "device = torch.device(\"mps\" if torch.backends.mps.is_available() else \"cpu\")\n",
    "\n",
    "# 名前を少しわかりやすく\n",
    "X_train = load_and_normalize(\"data/X_train.txt\")\n",
    "X_train = torch.tensor(X_train, dtype=torch.float32).to(device)\n",
    "\n",
    "Y_train = np.loadtxt(\"data/Y_train.txt\", delimiter=' ')\n",
    "Y_train = torch.tensor(Y_train, dtype=torch.long).to(device)\n",
    "\n",
    "X_valid = load_and_normalize(\"data/X_valid.txt\")\n",
    "X_valid = torch.tensor(X_valid, dtype=torch.float32).to(device)\n",
    "\n",
    "Y_valid = np.loadtxt(\"data/Y_valid.txt\", delimiter=' ')\n",
    "Y_valid = torch.tensor(Y_valid, dtype=torch.long).to(device)\n",
    "\n",
    "loss_fn = torch.nn.CrossEntropyLoss().to(device)\n",
    "writer = SummaryWriter(log_dir=\"log\")\n",
    "\n",
    "def acc(pred, label):\n",
    "    pred = torch.argmax(pred, axis=1).cpu().numpy()\n",
    "    label = label.cpu().numpy()\n",
    "    return (pred == label).mean()\n",
    "\n",
    "batch_sizes = [1, 2, 4, 8, 16, 32, 64, 128, 256]\n",
    "epoch_time_results = {}\n",
    "\n",
    "for B in batch_sizes:\n",
    "    W = torch.randn(300, 4, dtype=torch.float32, requires_grad=True, device=device)\n",
    "    optimizer = torch.optim.SGD([W], lr=0.1, momentum=0.9)\n",
    "\n",
    "    start_time = time.time()\n",
    "    \n",
    "    for epoch in range(10):\n",
    "        permutation = torch.randperm(X_train.size()[0], device=device)\n",
    "        \n",
    "        for i in range(0, X_train.size()[0], B):\n",
    "            indices = permutation[i:i+B]\n",
    "            batch_X, batch_Y = X_train[indices], Y_train[indices]\n",
    "            \n",
    "            scores = batch_X @ W\n",
    "            loss = loss_fn(scores, batch_Y)\n",
    "            \n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "        with torch.no_grad():\n",
    "            Y_pred = X_train @ W\n",
    "            train_loss = loss_fn(Y_pred, Y_train) \n",
    "            writer.add_scalar(f'Loss/train_batch_{B}', train_loss, epoch)\n",
    "            writer.add_scalar(f'Accuracy/train_batch_{B}', acc(Y_pred,Y_train), epoch)\n",
    "\n",
    "            Y_pred = X_valid @ W\n",
    "            valid_loss = loss_fn(Y_pred, Y_valid)\n",
    "            writer.add_scalar(f'Loss/valid_batch_{B}', valid_loss, epoch)\n",
    "            writer.add_scalar(f'Accuracy/valid_batch_{B}', acc(Y_pred,Y_valid), epoch)\n",
    "\n",
    "    end_time = time.time()\n",
    "    epoch_time_results[B] = end_time - start_time\n",
    "    print(f\"Batch size {B}: {epoch_time_results[B]} seconds\")\n",
    "\n",
    "    checkpoint = {\n",
    "        'epoch': epoch,\n",
    "        'model_state_dict': W.cpu(),\n",
    "        'optimizer_state_dict': optimizer.state_dict()\n",
    "    }\n",
    "    torch.save(checkpoint, f\"cp/batch_{B}_epoch_{epoch}.pt\")\n",
    "\n",
    "print(\"Training Accuracy:\", acc(X_train @ W, Y_train))\n",
    "print(\"Validation Accuracy:\", acc(X_valid @ W, Y_valid))\n",
    "print(\"Epoch time results:\", epoch_time_results)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "79. 多層ニューラルネットワーク\n",
    "\n",
    "問題78のコードを改変し，バイアス項の導入や多層化など，ニューラルネットワークの形状を変更しながら，高性能なカテゴリ分類器を構築せよ．"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Accuracy: 0.9396616541353383\n",
      "Validation Accuracy: 0.8902255639097745\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "\n",
    "# 3LP\n",
    "class MLP(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.w1 = nn.Linear(300, 600)\n",
    "        self.w2 = nn.Linear(600, 50)\n",
    "        self.w3 = nn.Linear(50, 4)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.w1(x))\n",
    "        x = F.relu(self.w2(x))\n",
    "        return self.w3(x)\n",
    "\n",
    "# 正規化\n",
    "def load_and_normalize(file_path):\n",
    "    data = np.loadtxt(file_path, delimiter=' ')\n",
    "    mean = np.mean(data, axis=0)\n",
    "    std = np.std(data, axis=0)\n",
    "    return (data - mean) / std\n",
    "\n",
    "# 正解率\n",
    "def acc(pred, label):\n",
    "    pred = torch.argmax(pred, axis=1).cpu().numpy()\n",
    "    label = label.cpu().numpy()\n",
    "    return (pred == label).mean()\n",
    "\n",
    "device = torch.device(\"mps\" if torch.backends.mps.is_available() else \"cpu\")\n",
    "\n",
    "# 読み込み と to Tensor\n",
    "X_train = load_and_normalize(\"data/X_train.txt\")\n",
    "X_train = torch.tensor(X_train, dtype=torch.float32).to(device)\n",
    "\n",
    "Y_train = np.loadtxt(\"data/Y_train.txt\", delimiter=' ')\n",
    "Y_train = torch.tensor(Y_train, dtype=torch.long).to(device)\n",
    "\n",
    "X_valid = load_and_normalize(\"data/X_valid.txt\")\n",
    "X_valid = torch.tensor(X_valid, dtype=torch.float32).to(device)\n",
    "\n",
    "Y_valid = np.loadtxt(\"data/Y_valid.txt\", delimiter=' ')\n",
    "Y_valid = torch.tensor(Y_valid, dtype=torch.long).to(device)\n",
    "\n",
    "batch_size = 2**10\n",
    "\n",
    "# 訓練データだけ先に作っておく\n",
    "train = DataLoader(TensorDataset(X_train,Y_train),batch_size=batch_size,shuffle=True)\n",
    "\n",
    "# インスタンス\n",
    "net = MLP().to(device)\n",
    "\n",
    "# loss_function, optimizerの設定\n",
    "loss_fn = nn.CrossEntropyLoss().to(device)\n",
    "optimizer = torch.optim.SGD(net.parameters(), lr=0.0001, momentum=0.9)\n",
    "\n",
    "# 一応学習曲線出すために\n",
    "writer = SummaryWriter(log_dir=\"log\")\n",
    "\n",
    "for epoch in range(3000):\n",
    "    for x, y in train:\n",
    "        optimizer.zero_grad()\n",
    "        scores = net(x)\n",
    "        loss = loss_fn(scores, y)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "    with torch.no_grad():\n",
    "        scores = net(X_train)\n",
    "        train_loss = loss_fn(scores, Y_train) \n",
    "        writer.add_scalar(f'Loss/train', train_loss, epoch)\n",
    "        writer.add_scalar(f'Accuracy/train', acc(scores,Y_train), epoch)\n",
    "\n",
    "        scores = net(X_valid)\n",
    "        valid_loss = loss_fn(scores, Y_valid)\n",
    "        writer.add_scalar(f'Loss/valid', valid_loss, epoch)\n",
    "        writer.add_scalar(f'Accuracy/valid', acc(scores,Y_valid), epoch)\n",
    "\n",
    "checkpoint = {\n",
    "    'epoch': epoch,\n",
    "    'model_state_dict': net.state_dict(),\n",
    "    'optimizer_state_dict': optimizer.state_dict()\n",
    "}\n",
    "torch.save(checkpoint, f\"cp/batch_{batch_size}_epoch_{epoch}.pt\")\n",
    "\n",
    "print(\"Training Accuracy:\", acc(net(X_train),Y_train))\n",
    "print(\"Validation Accuracy:\", acc(net(X_valid),Y_valid))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
