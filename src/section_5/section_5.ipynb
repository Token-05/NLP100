{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 762,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import graphviz"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "40. 係り受け解析結果の読み込み（形態素）\n",
    "\n",
    "形態素を表すクラスMorphを実装せよ．このクラスは表層形（surface），基本形（base），品詞（pos），品詞細分類1（pos1）をメンバ変数に持つこととする．さらに，係り受け解析の結果（ai.ja.txt.parsed）を読み込み，各文をMorphオブジェクトのリストとして表現し，冒頭の説明文の形態素列を表示せよ．"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 763,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['*', '0', '17D', '3/3', '0.388993\\n']\n",
      "['人工', '人工', '名詞', '一般']\n",
      "['知能', '知能', '名詞', '一般']\n",
      "['人工', '人工', '名詞', '一般']\n",
      "['知能', '知能', '名詞', '一般']\n"
     ]
    }
   ],
   "source": [
    "class Morph:\n",
    "    \n",
    "    def __init__(self,surface,base,pos,pos1):\n",
    "        self.surface = surface\n",
    "        self.base = base\n",
    "        self.pos = pos\n",
    "        self.pos1 = pos1\n",
    "\n",
    "    def return_parsed(self):\n",
    "        return [self.surface,self.base,self.pos,self.pos1]\n",
    "\n",
    "pattern = r'^\\* \\d+ -?[0-9]+[A-Z] \\d+/\\d+ -?\\d+\\.\\d+$'\n",
    "\n",
    "with open('ai.ja.txt.parsed','r') as lines:\n",
    "    for line in lines:\n",
    "        matchs = re.findall(pattern,line)\n",
    "        if line == 'EOS':\n",
    "            break\n",
    "        elif matchs:\n",
    "            match_list = line.split(' ')\n",
    "            if match_list[1] == '1':\n",
    "                break\n",
    "            print(match_list)\n",
    "        else:\n",
    "            line = line.split('\\t')\n",
    "            surface,sentences = line[0],line[1]\n",
    "            sentence = sentences.split(',')\n",
    "            (base,pos,pos1) = sentence[6],sentence[0],sentence[1]\n",
    "            m = Morph(surface=surface,base=base,pos=pos,pos1=pos1)\n",
    "            print(m.return_parsed())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "41. 係り受け解析結果の読み込み（文節・係り受け）\n",
    "\n",
    "40に加えて，文節を表すクラスChunkを実装せよ．このクラスは形態素（Morphオブジェクト）のリスト（morphs），係り先文節インデックス番号（dst），係り元文節インデックス番号のリスト（srcs）をメンバ変数に持つこととする．さらに，入力テキストの係り受け解析結果を読み込み，１文をChunkオブジェクトのリストとして表現し，冒頭の説明文の文節の文字列と係り先を表示せよ．本章の残りの問題では，ここで作ったプログラムを活用せよ．"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 764,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "インデックス番号：2937\n",
      "文節の文字列：発言し\n",
      "係り先のインデックス番号：2941\n",
      "\n",
      "インデックス番号：2938\n",
      "文節の文字列：伊勢田は\n",
      "係り先のインデックス番号：2941\n",
      "\n",
      "インデックス番号：2939\n",
      "文節の文字列：決着は\n",
      "係り先のインデックス番号：2940\n",
      "\n",
      "インデックス番号：2940\n",
      "文節の文字列：つかないでしょうねと\n",
      "係り先のインデックス番号：2941\n",
      "\n",
      "インデックス番号：2941\n",
      "文節の文字列：答えている\n",
      "係り先のインデックス番号：-1\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from typing import Any\n",
    "\n",
    "\n",
    "class Chunk:\n",
    "    \n",
    "    def __init__(self, morphs, dst, srcs, chunk_id):\n",
    "        self.morphs = morphs\n",
    "        self.dst = dst\n",
    "        self.srcs = srcs\n",
    "        self.chunk_id = chunk_id\n",
    "        # 43 文節における名詞・動詞の有無フラグ\n",
    "        self.has_noun = False\n",
    "        self.has_verb = False\n",
    "    \n",
    "    def return_string_dependency(self):\n",
    "        surface = []\n",
    "        for morph in self.morphs:\n",
    "            surface.append(morph.surface)\n",
    "        return \"\".join(surface),self.dst,self.chunk_id\n",
    "    \n",
    "    def return_string_sources(self):\n",
    "        return self.srcs,self.chunk_id\n",
    "\n",
    "pattern = r'^\\* \\d+ -?[0-9]+[A-Z] \\d+/\\d+ -?\\d+\\.\\d+$'\n",
    "morphs = []\n",
    "chunks = []\n",
    "dst = None\n",
    "srcs = []\n",
    "chunk_id = -1\n",
    "has_noun = False\n",
    "has_verb = False\n",
    "\n",
    "with open('ai.ja.txt.parsed','r') as lines:\n",
    "    for line in lines:\n",
    "        matchs = re.findall(pattern,line)\n",
    "        if line == 'EOS':\n",
    "            # 最後まで行ったら終了\n",
    "            c = Chunk(morphs,dst,srcs,chunk_id)\n",
    "            chunks.append(c)\n",
    "            if has_noun:\n",
    "                c.has_noun = True\n",
    "            elif has_verb:\n",
    "                c.has_verb = True\n",
    "            break\n",
    "        elif matchs:\n",
    "            match_list = line.split(' ')\n",
    "            # 一番最初はやらない\n",
    "            if chunk_id < 0:\n",
    "                chunk_id = 0\n",
    "            # 2回目以降\n",
    "            else:\n",
    "                c = Chunk(morphs,dst,srcs,chunk_id)\n",
    "                chunks.append(c)\n",
    "                if has_noun:\n",
    "                    c.has_noun = True\n",
    "                elif has_verb:\n",
    "                    c.has_verb = True\n",
    "            morphs = []\n",
    "            dst = int(match_list[2].replace(\"D\", \"\"))\n",
    "            chunk_id = int(match_list[1])\n",
    "            srcs=[]\n",
    "            has_noun = False\n",
    "            has_verb = False\n",
    "        else:\n",
    "            line = line.split('\\t')\n",
    "            (surface,sentences) = line[0],line[1]\n",
    "            sentence = sentences.split(',')\n",
    "            (base,pos,pos1) = sentence[6],sentence[0],sentence[1]\n",
    "            # 42 記号削除\n",
    "            if pos == '記号':\n",
    "                morphs.append(Morph('',base,pos,pos1))\n",
    "            else:\n",
    "                morphs.append(Morph(surface,base,pos,pos1))   \n",
    "            # 43 文節における名詞・動詞の有無\n",
    "            if pos == '名詞':\n",
    "                has_noun = True\n",
    "            elif pos == '動詞':\n",
    "                has_verb = True\n",
    "                     \n",
    "\n",
    "for chunk in chunks[-5:]:\n",
    "    (sentence,dst,id) = chunk.return_string_dependency()\n",
    "    print(f\"インデックス番号：{id}\\n文節の文字列：{sentence}\\n係り先のインデックス番号：{dst}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "42. 係り元と係り先の文節の表示\n",
    "\n",
    "係り元の文節と係り先の文節のテキストをタブ区切り形式ですべて抽出せよ．ただし，句読点などの記号は出力しないようにせよ．"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 765,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[]\t語\n",
      "[]\t語\n",
      "[]\tエーアイとは\n",
      "['AI']\t語\n",
      "[]\tという\n",
      "['計算']\t道具を\n",
      "[]\t道具を\n",
      "[]\tという\n",
      "['コンピュータ']\t道具を\n"
     ]
    }
   ],
   "source": [
    "# srcsにデータ挿入\n",
    "for chunk in range(len(chunks)):\n",
    "    (sentence,dst,id) = chunks[chunk].return_string_dependency()\n",
    "    chunks[dst].srcs.append(id)\n",
    "\n",
    "# 一部表示\n",
    "for chunk in range(len(chunks)//300):\n",
    "    source = []\n",
    "    (srcs, id) = chunks[chunk].return_string_sources()\n",
    "    for srcs_index in srcs:\n",
    "        if srcs_index < 0:\n",
    "            continue\n",
    "        (mor, _, _)=chunks[srcs_index].return_string_dependency()\n",
    "        source.append(mor)\n",
    "    (main, dst, _)=chunks[chunk].return_string_dependency()\n",
    "    dst_mor = None\n",
    "    if dst > 0:\n",
    "        (dst_mor, _, _)=chunks[dst].return_string_dependency()\n",
    "    print(f\"{source}\\t{dst_mor}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "43. 名詞を含む文節が動詞を含む文節に係るものを抽出\n",
    "\n",
    "名詞を含む文節が，動詞を含む文節に係るとき，これらをタブ区切り形式で抽出せよ．ただし，句読点などの記号は出力しないようにせよ．"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 766,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "道具を\t用いて\n",
      "一分野を\t指す\n",
      "知的行動を\t代わって\n",
      "人間に\t代わって\n",
      "コンピューターに\t行わせる\n",
      "研究分野とも\tされる\n",
      "解説で\t述べている\n",
      "佐藤理史は\t述べている\n",
      "次のように\t述べている\n",
      "技術ソフトウェアコンピュータシステム\tある\n",
      "応用例は\tある\n"
     ]
    }
   ],
   "source": [
    "for chunk in range(len(chunks)//50):\n",
    "    (mor_n, dst, _) = chunks[chunk].return_string_dependency()\n",
    "    (mor_v, _, _) = chunks[dst].return_string_dependency()\n",
    "    if dst > 0 and chunks[chunk].has_noun and chunks[dst].has_verb:\n",
    "        print(f\"{mor_n}\\t{mor_v}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "44. 係り受け木の可視化\n",
    "\n",
    "与えられた文の係り受け木を有向グラフとして可視化せよ．可視化には，[Graphviz](http://www.graphviz.org/)等を用いるとよい．\n",
    "(今回用いたインターフェースは[これ](https://graphviz.readthedocs.io/en/stable/manual.html))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 767,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/svg+xml": [
       "<?xml version=\"1.0\" encoding=\"UTF-8\" standalone=\"no\"?>\n",
       "<!DOCTYPE svg PUBLIC \"-//W3C//DTD SVG 1.1//EN\"\n",
       " \"http://www.w3.org/Graphics/SVG/1.1/DTD/svg11.dtd\">\n",
       "<!-- Generated by graphviz version 9.0.0 (20230911.1827)\n",
       " -->\n",
       "<!-- Pages: 1 -->\n",
       "<svg width=\"466pt\" height=\"260pt\"\n",
       " viewBox=\"0.00 0.00 465.76 260.00\" xmlns=\"http://www.w3.org/2000/svg\" xmlns:xlink=\"http://www.w3.org/1999/xlink\">\n",
       "<g id=\"graph0\" class=\"graph\" transform=\"scale(1 1) rotate(0) translate(4 256)\">\n",
       "<polygon fill=\"white\" stroke=\"none\" points=\"-4,4 -4,-256 461.76,-256 461.76,4 -4,4\"/>\n",
       "<!-- ジョンマッカーシーは -->\n",
       "<g id=\"node1\" class=\"node\">\n",
       "<title>ジョンマッカーシーは</title>\n",
       "<ellipse fill=\"none\" stroke=\"black\" cx=\"102.53\" cy=\"-90\" rx=\"102.53\" ry=\"18\"/>\n",
       "<text text-anchor=\"middle\" x=\"102.53\" y=\"-84.2\" font-family=\"Times,serif\" font-size=\"14.00\">ジョンマッカーシーは</text>\n",
       "</g>\n",
       "<!-- 作り出した -->\n",
       "<g id=\"node2\" class=\"node\">\n",
       "<title>作り出した</title>\n",
       "<ellipse fill=\"none\" stroke=\"black\" cx=\"261.53\" cy=\"-18\" rx=\"56.71\" ry=\"18\"/>\n",
       "<text text-anchor=\"middle\" x=\"261.53\" y=\"-12.2\" font-family=\"Times,serif\" font-size=\"14.00\">作り出した</text>\n",
       "</g>\n",
       "<!-- ジョンマッカーシーは&#45;&gt;作り出した -->\n",
       "<g id=\"edge1\" class=\"edge\">\n",
       "<title>ジョンマッカーシーは&#45;&gt;作り出した</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M139.41,-72.76C163.17,-62.3 194.14,-48.67 218.95,-37.75\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"220.03,-41.09 227.78,-33.86 217.21,-34.69 220.03,-41.09\"/>\n",
       "</g>\n",
       "<!-- AIに関する -->\n",
       "<g id=\"node3\" class=\"node\">\n",
       "<title>AIに関する</title>\n",
       "<ellipse fill=\"none\" stroke=\"black\" cx=\"251.53\" cy=\"-234\" rx=\"59.54\" ry=\"18\"/>\n",
       "<text text-anchor=\"middle\" x=\"251.53\" y=\"-228.95\" font-family=\"Times,serif\" font-size=\"14.00\">AIに関する</text>\n",
       "</g>\n",
       "<!-- 最初の -->\n",
       "<g id=\"node4\" class=\"node\">\n",
       "<title>最初の</title>\n",
       "<ellipse fill=\"none\" stroke=\"black\" cx=\"251.53\" cy=\"-162\" rx=\"38.18\" ry=\"18\"/>\n",
       "<text text-anchor=\"middle\" x=\"251.53\" y=\"-156.2\" font-family=\"Times,serif\" font-size=\"14.00\">最初の</text>\n",
       "</g>\n",
       "<!-- AIに関する&#45;&gt;最初の -->\n",
       "<g id=\"edge2\" class=\"edge\">\n",
       "<title>AIに関する&#45;&gt;最初の</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M251.53,-215.7C251.53,-208.41 251.53,-199.73 251.53,-191.54\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"255.03,-191.62 251.53,-181.62 248.03,-191.62 255.03,-191.62\"/>\n",
       "</g>\n",
       "<!-- 会議で -->\n",
       "<g id=\"node5\" class=\"node\">\n",
       "<title>会議で</title>\n",
       "<ellipse fill=\"none\" stroke=\"black\" cx=\"261.53\" cy=\"-90\" rx=\"38.18\" ry=\"18\"/>\n",
       "<text text-anchor=\"middle\" x=\"261.53\" y=\"-84.2\" font-family=\"Times,serif\" font-size=\"14.00\">会議で</text>\n",
       "</g>\n",
       "<!-- 最初の&#45;&gt;会議で -->\n",
       "<g id=\"edge3\" class=\"edge\">\n",
       "<title>最初の&#45;&gt;会議で</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M254,-143.7C255.05,-136.32 256.31,-127.52 257.49,-119.25\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"260.92,-120 258.87,-109.6 253.99,-119.01 260.92,-120\"/>\n",
       "</g>\n",
       "<!-- 会議で&#45;&gt;作り出した -->\n",
       "<g id=\"edge4\" class=\"edge\">\n",
       "<title>会議で&#45;&gt;作り出した</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M261.53,-71.7C261.53,-64.41 261.53,-55.73 261.53,-47.54\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"265.03,-47.62 261.53,-37.62 258.03,-47.62 265.03,-47.62\"/>\n",
       "</g>\n",
       "<!-- 人工知能という -->\n",
       "<g id=\"node6\" class=\"node\">\n",
       "<title>人工知能という</title>\n",
       "<ellipse fill=\"none\" stroke=\"black\" cx=\"382.53\" cy=\"-162\" rx=\"75.23\" ry=\"18\"/>\n",
       "<text text-anchor=\"middle\" x=\"382.53\" y=\"-156.2\" font-family=\"Times,serif\" font-size=\"14.00\">人工知能という</text>\n",
       "</g>\n",
       "<!-- 用語を -->\n",
       "<g id=\"node7\" class=\"node\">\n",
       "<title>用語を</title>\n",
       "<ellipse fill=\"none\" stroke=\"black\" cx=\"373.53\" cy=\"-90\" rx=\"38.18\" ry=\"18\"/>\n",
       "<text text-anchor=\"middle\" x=\"373.53\" y=\"-84.2\" font-family=\"Times,serif\" font-size=\"14.00\">用語を</text>\n",
       "</g>\n",
       "<!-- 人工知能という&#45;&gt;用語を -->\n",
       "<g id=\"edge5\" class=\"edge\">\n",
       "<title>人工知能という&#45;&gt;用語を</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M380.3,-143.7C379.36,-136.32 378.22,-127.52 377.16,-119.25\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"380.67,-119.08 375.92,-109.61 373.72,-119.97 380.67,-119.08\"/>\n",
       "</g>\n",
       "<!-- 用語を&#45;&gt;作り出した -->\n",
       "<g id=\"edge6\" class=\"edge\">\n",
       "<title>用語を&#45;&gt;作り出した</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M351.13,-75C335.3,-65.11 313.77,-51.65 295.79,-40.41\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"297.84,-37.57 287.5,-35.24 294.13,-43.5 297.84,-37.57\"/>\n",
       "</g>\n",
       "</g>\n",
       "</svg>\n"
      ],
      "text/plain": [
       "<graphviz.graphs.Digraph at 0x10cdd3a90>"
      ]
     },
     "execution_count": 767,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# これだと枝が重複するかも\n",
    "# 固有IDあった方が良い\n",
    "branchs = []\n",
    "d = graphviz.Digraph()\n",
    "# 「ジョン・マッカーシーはAIに関する最初の会議で人工知能という用語を作り出した。」\n",
    "for chunk in range(357,363):\n",
    "    (tail, dst, _) = chunks[chunk].return_string_dependency()\n",
    "    (head, _, _) = chunks[dst].return_string_dependency()\n",
    "    if dst > 0:\n",
    "        d.edge(tail,head)\n",
    "d"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "45. 動詞の格パターンの抽出\n",
    "\n",
    "今回用いている文章をコーパスと見なし，日本語の述語が取りうる格を調査したい． 動詞を述語，動詞に係っている文節の助詞を格と考え，述語と格をタブ区切り形式で出力せよ． ただし，出力は以下の仕様を満たすようにせよ．\n",
    "\n",
    "- 動詞を含む文節において，最左の動詞の基本形を述語とする\n",
    "- 述語に係る助詞を格とする\n",
    "- 述語に係る助詞（文節）が複数あるときは，すべての助詞をスペース区切りで辞書順に並べる\n",
    "\n",
    "「ジョン・マッカーシーはAIに関する最初の会議で人工知能という用語を作り出した。」という例文を考える． この文は「作り出す」という１つの動詞を含み，「作り出す」に係る文節は「ジョン・マッカーシーは」，「会議で」，「用語を」であると解析された場合は，次のような出力になるはずである．\n",
    "\n",
    "```zsh\n",
    "作り出す\tで は を\n",
    "```\n",
    "\n",
    "このプログラムの出力をファイルに保存し，以下の事項をUNIXコマンドを用いて確認せよ．\n",
    "\n",
    "- コーパス中で頻出する述語と格パターンの組み合わせ\n",
    "- 「行う」「なる」「与える」という動詞の格パターン（コーパス中で出現頻度の高い順に並べよ）\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 768,
   "metadata": {},
   "outputs": [],
   "source": [
    "f = open('verb_kaku.txt','w')\n",
    "# 動詞に助詞がかかる場合のみ取り出す\n",
    "for chunk in range(len(chunks)):\n",
    "    if chunks[chunk].has_verb:\n",
    "        verb = None\n",
    "        morphs = chunks[chunk].morphs\n",
    "        for morph in morphs:\n",
    "            (_, base, pos, _) = morph.return_parsed()\n",
    "            if pos == '動詞':\n",
    "                verb = base\n",
    "                break\n",
    "        kakus = []\n",
    "        (srcs, id) = chunks[chunk].return_string_sources()\n",
    "        for srcs_index in srcs:\n",
    "            if srcs_index < 0:\n",
    "                continue\n",
    "            kaku = ''\n",
    "            morphs=chunks[srcs_index].morphs\n",
    "            for morph in morphs:\n",
    "                (_, base, pos, _) = morph.return_parsed()\n",
    "                if pos == '助詞':\n",
    "                    kaku = base\n",
    "            kakus.append(kaku)\n",
    "        # print(verb+'\\t',*kakus,sep=' ')\n",
    "        f.write(verb+'\\t'+' '.join(sorted(kakus))+'\\n')\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- コーパス中で頻出する述語と格パターンの組み合わせ\n",
    "\n",
    "```zsh\n",
    "\n",
    "% sort verb_kaku.txt | uniq -c | sort -n -r | head\n",
    "  11 よる       に\n",
    "   7 れる       と\n",
    "   6 する       と\n",
    "   5 用いる     を\n",
    "   5 基づく     に\n",
    "   4 行う       を\n",
    "   4 ある       が\n",
    "   3 関わる     も\n",
    "   3 向ける     に\n",
    "   3 受ける     を\n",
    "\n",
    "```\n",
    "\n",
    "- 「行う」「なる」「与える」という動詞の格パターン（コーパス中で出現頻度の高い順に並べよ）\n",
    "\n",
    "```zsh\n",
    "\n",
    "% grep \"^行う\\t\" verb_kaku.txt | sort | uniq -c | sort -n -r | head\n",
    "   4 行う       を\n",
    "   1 行う       に を を\n",
    "   1 行う       で に を\n",
    "   1 行う        は を\n",
    "   1 行う        で を\n",
    "\n",
    "% grep \"^なる\\t\" verb_kaku.txt | sort | uniq -c | sort -n -r | head\n",
    "   2 なる       が と\n",
    "   1 なる       が にとって は\n",
    "   1 なる       と など\n",
    "   1 なる       が て と\n",
    "   1 なる       は を\n",
    "   1 なる       に は\n",
    "   1 なる       が に\n",
    "   1 なる       に\n",
    "   1 なる       と\n",
    "   1 なる        が で と に は は\n",
    "\n",
    "% grep \"^与える\\t\" verb_kaku.txt | sort | uniq -c | sort -n -r | head\n",
    "   1 与える     に は を\n",
    "   1 与える     が に\n",
    "\n",
    "```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "46. 動詞の格フレーム情報の抽出\n",
    "\n",
    "45のプログラムを改変し，述語と格パターンに続けて項（述語に係っている文節そのもの）をタブ区切り形式で出力せよ．45の仕様に加えて，以下の仕様を満たすようにせよ．\n",
    "\n",
    "- 項は述語に係っている文節の単語列とする（末尾の助詞を取り除く必要はない）\n",
    "- 述語に係る文節が複数あるときは，助詞と同一の基準・順序でスペース区切りで並べる\n",
    "\n",
    "「ジョン・マッカーシーはAIに関する最初の会議で人工知能という用語を作り出した。」という例文を考える． この文は「作り出す」という１つの動詞を含み，「作り出す」に係る文節は「ジョン・マッカーシーは」，「会議で」，「用語を」であると解析された場合は，次のような出力になるはずである．\n",
    "\n",
    "```zsh\n",
    "作り出す\tで は を\t会議で ジョンマッカーシーは 用語を\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 769,
   "metadata": {},
   "outputs": [],
   "source": [
    "f = open('verb_kaku_kou.txt','w')\n",
    "# 項を追加\n",
    "for chunk in range(len(chunks)):\n",
    "    if chunks[chunk].has_verb:\n",
    "        verb = None\n",
    "        morphs = chunks[chunk].morphs\n",
    "        for morph in morphs:\n",
    "            (_, base, pos, _) = morph.return_parsed()\n",
    "            if pos == '動詞':\n",
    "                verb = base\n",
    "                break\n",
    "        kakus, kous = [], []\n",
    "        (srcs, id) = chunks[chunk].return_string_sources()\n",
    "        for srcs_index in srcs:\n",
    "            if srcs_index < 0:\n",
    "                continue\n",
    "            kaku, kou = '', ''\n",
    "            morphs = chunks[srcs_index].morphs\n",
    "            (mor, _, _) = chunks[srcs_index].return_string_dependency()\n",
    "            for morph in morphs:\n",
    "                (_, base, pos, _) = morph.return_parsed()\n",
    "                if pos == '助詞':\n",
    "                    kaku = base\n",
    "                    kou = mor\n",
    "            kakus.append([kaku, kou])\n",
    "        kakus = sorted(kakus, key=lambda x: x[0])\n",
    "        f.write(verb+'\\t'+' '.join([x[0] for x in kakus])+'\\t'+' '.join([x[1] for x in kakus])+'\\n')\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 出力結果における先頭10行\n",
    "\n",
    "```txt\n",
    "用いる\tを\t道具を\n",
    "指す\tを\t一分野を\n",
    "代わる\tに を\t人間に 知的行動を\n",
    "せる\tて に\t代わって コンピューターに\n",
    "れる\tも\t研究分野とも\n",
    "いる\tで に は\t解説で 次のように 佐藤理史は\n",
    "ある\t が は\t 画像認識等が 応用例は\n",
    "用いる\tを\t記号処理を\n",
    "する\tと を\t主体と 記述を\n",
    "いる\t でも は\t 意味あいでも 現在では\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "47. 機能動詞構文のマイニング\n",
    "\n",
    "動詞のヲ格にサ変接続名詞が入っている場合のみに着目したい．46のプログラムを以下の仕様を満たすように改変せよ．\n",
    "\n",
    "- 「サ変接続名詞+を（助詞）」で構成される文節が動詞に係る場合のみを対象とする\n",
    "- 述語は「サ変接続名詞+を+動詞の基本形」とし，文節中に複数の動詞があるときは，最左の動詞を用いる\n",
    "- 述語に係る助詞（文節）が複数あるときは，すべての助詞をスペース区切りで辞書順に並べる\n",
    "- 述語に係る文節が複数ある場合は，すべての項をスペース区切りで並べる（助詞の並び順と揃えよ）\n",
    "\n",
    "例えば「また、自らの経験を元に学習を行う強化学習という手法もある。」という文から，以下の出力が得られるはずである．\n",
    "\n",
    "```\n",
    "学習を行う\tに を\t元に 経験を\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 770,
   "metadata": {},
   "outputs": [],
   "source": [
    "f = open('sahen_wo_verb.txt','w')\n",
    "# 項を追加\n",
    "for chunk in range(len(chunks)):\n",
    "    if chunks[chunk].has_verb:\n",
    "        verb = None\n",
    "        morphs = chunks[chunk].morphs\n",
    "        for morph in morphs:\n",
    "            (_, base, pos, _) = morph.return_parsed()\n",
    "            if pos == '動詞':\n",
    "                verb = base\n",
    "                break\n",
    "        kakus, kous = [], []\n",
    "        sahen_wo = ''\n",
    "        sahen_wo_verb = None\n",
    "        (srcs, id) = chunks[chunk].return_string_sources()\n",
    "        for srcs_index in srcs:\n",
    "            if srcs_index < 0:\n",
    "                continue\n",
    "            kaku, kou = '', ''\n",
    "            morphs = chunks[srcs_index].morphs\n",
    "            (mor, _, _) = chunks[srcs_index].return_string_dependency()\n",
    "            if len(morphs) > 1 :\n",
    "                if morphs[0].pos1 == 'サ変接続' and morphs[1].surface == 'を' and morphs[1].pos == '助詞':\n",
    "                    sahen_wo = morphs[0].surface + morphs[1].surface\n",
    "                    sahen_wo_verb = morphs[0].surface + morphs[1].surface + verb\n",
    "            for morph in morphs:\n",
    "                (_, base, pos, _) = morph.return_parsed()\n",
    "                if pos == '助詞':\n",
    "                    kaku = base\n",
    "                    kou = mor\n",
    "            kakus.append([kaku, kou])\n",
    "        for i in range(len(kakus)):\n",
    "            if kakus[i][1] == sahen_wo:\n",
    "                kakus.pop(i)\n",
    "                break\n",
    "        kakus = sorted(kakus, key=lambda x: x[0])\n",
    "        if sahen_wo_verb:\n",
    "            f.write(sahen_wo_verb+'\\t'+' '.join([x[0] for x in kakus])+'\\t'+' '.join([x[1] for x in kakus])+'\\n')\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 出力結果\n",
    "\n",
    "```txt\n",
    "記述をする\tと\t主体と\n",
    "注目を集める\t   が\t   サポートベクターマシンが\n",
    "学習を行う\tに を\t元に 経験を\n",
    "流行を超える\t\t\n",
    "学習を繰り返す\t\t\n",
    "進化を見せる\t て て において は\t 活躍している 加えて 生成技術において 敵対的生成ネットワークは\n",
    "開発を行う\t は\t エイダ・ラブレスは\n",
    "処理を行う\t\t\n",
    "研究を進める\tて\t費やして\n",
    "注目を集める\t  から は\t  ことから ファジィは\n",
    "成功を受ける\t\t\n",
    "進歩を担う\t\t\n",
    "研究を続ける\tが て\tジェフ・ホーキンスが 向けて\n",
    "注目を集める\tに\t急速に\n",
    "普及を受ける\t\t\n",
    "投資を行う\tで に\t民間企業主導で 全世界的に\n",
    "探索を行う\t で\t 無報酬で\n",
    "研究を行う\t て\t 始めており\n",
    "実験をする\t\t\n",
    "投資をする\t に は\t 2022年までに 韓国は\n",
    "反乱を起こす\tて に対して\t於いて 人間に対して\n",
    "弾圧を併せ持つ\t\t\n",
    "監視を行う\t に まで\t 人工知能に 歩行者まで\n",
    "差別を認める\t\t\n",
    "展開を変える\t\t\n",
    "判断を介す\tから\t観点から\n",
    "禁止を求める\t が は\t ヒューマン・ライツ・ウォッチが 4月には\n",
    "運用をめぐる\t\t\n",
    "試験を行う\t\t\n",
    "追及を受ける\t て で と とともに は\t 暴露されており 整合性で 拒否すると とともに 公聴会では\n",
    "話をする\t は\t 哲学者は\n",
    "議論を行う\t  まで\t  これまで\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "48. 名詞から根へのパスの抽出\n",
    "\n",
    "文中のすべての名詞を含む文節に対し，その文節から構文木の根に至るパスを抽出せよ． ただし，構文木上のパスは以下の仕様を満たすものとする．\n",
    "\n",
    "- 各文節は（表層形の）形態素列で表現する\n",
    "- パスの開始文節から終了文節に至るまで，各文節の表現を” -> “で連結する\n",
    "\n",
    "「ジョン・マッカーシーはAIに関する最初の会議で人工知能という用語を作り出した。」という例文を考える． CaboChaを係り受け解析に用いた場合，次のような出力が得られると思われる．\n",
    "\n",
    "```zsh\n",
    "ジョンマッカーシーは -> 作り出した\n",
    "AIに関する -> 最初の -> 会議で -> 作り出した\n",
    "最初の -> 会議で -> 作り出した\n",
    "会議で -> 作り出した\n",
    "人工知能という -> 用語を -> 作り出した\n",
    "用語を -> 作り出した\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 771,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ジョンマッカーシーは -> 作り出した\n",
      "AIに関する -> 最初の -> 会議で -> 作り出した\n",
      "最初の -> 会議で -> 作り出した\n",
      "会議で -> 作り出した\n",
      "人工知能という -> 用語を -> 作り出した\n",
      "用語を -> 作り出した\n"
     ]
    }
   ],
   "source": [
    "start, end = 357, 363\n",
    "\n",
    "for chunk in range(start,end):\n",
    "    dst, col = chunk, []\n",
    "    while start <= dst and dst <= end:\n",
    "        sur, dst, _ =  chunks[dst].return_string_dependency()\n",
    "        col.append(sur)\n",
    "    print(\" -> \".join(col))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "49. 名詞間の係り受けパスの抽出Permalink\n",
    "\n",
    "文中のすべての名詞句のペアを結ぶ最短係り受けパスを抽出せよ．ただし，名詞句ペアの文節番号がi\n",
    "とj（i < j）のとき，係り受けパスは以下の仕様を満たすものとする．\n",
    "\n",
    "- 問題48と同様に，パスは開始文節から終了文節に至るまでの各文節の表現（表層形の形態素列）を” -> “で連結して表現する\n",
    "- 文節iとjに含まれる名詞句はそれぞれ，XとYに置換する\n",
    "\n",
    "また，係り受けパスの形状は，以下の2通りが考えられる．\n",
    "\n",
    "- 文節iから構文木の根に至る経路上に文節jが存在する場合: 文節iから文節jのパスを表示\n",
    "- 上記以外で，文節iと文節jから構文木の根に至る経路上で共通の文節kで交わる場合: 文節iから文節kに至る直前のパスと文節jから文節kに至る直前までのパス，文節kの内容を” | “で連結して表示\n",
    "\n",
    "「ジョン・マッカーシーはAIに関する最初の会議で人工知能という用語を作り出した。」という例文を考える． CaboChaを係り受け解析に用いた場合，次のような出力が得られると思われる．\n",
    "\n",
    "```zsh\n",
    "Xは | Yに関する -> 最初の -> 会議で | 作り出した\n",
    "Xは | Yの -> 会議で | 作り出した\n",
    "Xは | Yで | 作り出した\n",
    "Xは | Yという -> 用語を | 作り出した\n",
    "Xは | Yを | 作り出した\n",
    "Xに関する -> Yの\n",
    "Xに関する -> 最初の -> Yで\n",
    "Xに関する -> 最初の -> 会議で | Yという -> 用語を | 作り出した\n",
    "Xに関する -> 最初の -> 会議で | Yを | 作り出した\n",
    "Xの -> Yで\n",
    "Xの -> 会議で | Yという -> 用語を | 作り出した\n",
    "Xの -> 会議で | Yを | 作り出した\n",
    "Xで | Yという -> 用語を | 作り出した\n",
    "Xで | Yを | 作り出した\n",
    "Xという -> Yを\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 772,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Xは | Yに関する -> 最初の -> 会議で | 作り出した\n",
      "Xは | Yの -> 会議で | 作り出した\n",
      "Xは | Yで | 作り出した\n",
      "Xは | Yという -> 用語を | 作り出した\n",
      "Xは | Yを | 作り出した\n",
      "Xに関する -> Yの\n",
      "Xに関する -> 最初の -> Yで\n",
      "Xに関する -> 最初の -> 会議で | Yという -> 用語を | 作り出した\n",
      "Xに関する -> 最初の -> 会議で | Yを | 作り出した\n",
      "Xの -> Yで\n",
      "Xの -> 会議で | Yという -> 用語を | 作り出した\n",
      "Xの -> 会議で | Yを | 作り出した\n",
      "Xで | Yという -> 用語を | 作り出した\n",
      "Xで | Yを | 作り出した\n",
      "Xという -> Yを\n"
     ]
    }
   ],
   "source": [
    "start, end = 357, 363\n",
    "\n",
    "def return_path(root_i,root_j):\n",
    "\n",
    "    new_i, new_j = [],[]\n",
    "    for i,clause in enumerate(root_i):\n",
    "        bunnsetu = ''\n",
    "        mor, _, id = clause[1].return_string_dependency()\n",
    "        if i == 0 :\n",
    "            for morph in clause[1].morphs:\n",
    "                if morph.pos == '名詞':\n",
    "                    if 'X' in bunnsetu:\n",
    "                        continue\n",
    "                    bunnsetu += 'X'\n",
    "                else:\n",
    "                    bunnsetu += morph.surface\n",
    "        else:\n",
    "            bunnsetu += mor\n",
    "        new_i.append((id,bunnsetu))\n",
    "\n",
    "    j_id = int()\n",
    "    include_Y = tuple()\n",
    "    for j,clause in enumerate(root_j):\n",
    "        bunnsetu = ''\n",
    "        mor, _, id = clause[1].return_string_dependency()\n",
    "        if j == 0 :\n",
    "            for morph in clause[1].morphs:\n",
    "                if morph.pos == '名詞':\n",
    "                    if 'Y' in bunnsetu:\n",
    "                        continue\n",
    "                    bunnsetu += 'Y'\n",
    "                    j_id = id\n",
    "                else:\n",
    "                    bunnsetu += morph.surface\n",
    "                include_Y = (id,bunnsetu)\n",
    "        else:\n",
    "            bunnsetu += mor\n",
    "        new_j.append((id,bunnsetu))\n",
    "    \n",
    "    # 同じ文節があった場合、「Y」に変えてしまう\n",
    "    for idx in range(len(new_i)):\n",
    "        if new_i[idx][0] == j_id:\n",
    "            new_i[idx] = include_Y\n",
    "            break\n",
    "\n",
    "    roots = sorted(set(new_i+new_j),key=lambda x:x[0])\n",
    "    \n",
    "    roots = [list(r) for r in roots]\n",
    "    new_i = [list(i) for i in new_i]\n",
    "    new_j = [list(j) for j in new_j]\n",
    "    return roots, new_i, new_j\n",
    "\n",
    "def research_list(l,x):\n",
    "    return l.index(x) if x in l else None\n",
    "\n",
    "for i in range(start,end-1):\n",
    "    for j in range(i+1,end):\n",
    "        chunks_i, chunks_j = chunks[i], chunks[j]\n",
    "        if chunks_i.has_noun and chunks_j.has_noun:\n",
    "            root_i, root_j, dst_i, dst_j = [], [], i, j\n",
    "            while start <= dst_i and dst_i <= end:\n",
    "                mor_i = chunks[dst_i]\n",
    "                _, dst_i, id_i =  chunks[dst_i].return_string_dependency()\n",
    "                root_i.append((id_i,mor_i))\n",
    "            while start <= dst_j and dst_j <= end:\n",
    "                mor_j = chunks[dst_j]\n",
    "                _, dst_j, id_j =  chunks[dst_j].return_string_dependency()\n",
    "                root_j.append((id_j,mor_j))\n",
    "            roots, new_i, new_j  = return_path(root_i,root_j)\n",
    "            for root in roots:\n",
    "                idx = research_list(new_i,root)\n",
    "                jdx = research_list(new_j,root)\n",
    "                if idx == None:\n",
    "                    new_j[jdx].append(' -> ')\n",
    "                elif jdx == None:\n",
    "                    new_i[idx].append(' -> ')\n",
    "                else:\n",
    "                    if 'Y' in root[1]:\n",
    "                        tail = new_i[:idx]+new_j[:jdx+1]\n",
    "                        break\n",
    "                    if len(new_i[idx-1]) > 2:\n",
    "                        new_i[idx-1][2] = ' | '\n",
    "                    else:\n",
    "                        new_i[idx-1].append(' | ')\n",
    "                    if len(new_j[jdx-1]) > 2:\n",
    "                        new_j[jdx-1][2] = ' | '\n",
    "                    else:\n",
    "                        new_j[jdx-1].append(' | ')\n",
    "                tail = new_i[:-1]+new_j\n",
    "            \n",
    "            tail[-1].append('')\n",
    "            col = ''\n",
    "            for t in sorted(tail):\n",
    "                col += t[1]+t[2]\n",
    "            print(col)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
