{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import graphviz"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "40. 係り受け解析結果の読み込み（形態素）\n",
    "\n",
    "形態素を表すクラスMorphを実装せよ．このクラスは表層形（surface），基本形（base），品詞（pos），品詞細分類1（pos1）をメンバ変数に持つこととする．さらに，係り受け解析の結果（ai.ja.txt.parsed）を読み込み，各文をMorphオブジェクトのリストとして表現し，冒頭の説明文の形態素列を表示せよ．"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['*', '0', '17D', '3/3', '0.388993\\n']\n",
      "['人工', '人工', '名詞', '一般']\n",
      "['知能', '知能', '名詞', '一般']\n",
      "['人工', '人工', '名詞', '一般']\n",
      "['知能', '知能', '名詞', '一般']\n"
     ]
    }
   ],
   "source": [
    "class Morph:\n",
    "    \n",
    "    # イニシャライザを上記の通り設定\n",
    "    def __init__(self,surface,base,pos,pos1):\n",
    "        \n",
    "        self.surface = surface\n",
    "        self.base = base\n",
    "        self.pos = pos\n",
    "        self.pos1 = pos1\n",
    "\n",
    "    # 形態素列をリストで返す\n",
    "    def return_parsed(self):\n",
    "\n",
    "        return [self.surface,self.base,self.pos,self.pos1]\n",
    "\n",
    "# 文節ごとに係先の情報や自身のidを保持しているので、パターン化\n",
    "pattern = r'^\\* \\d+ -?[0-9]+[A-Z] \\d+/\\d+ -?\\d+\\.\\d+$'\n",
    "\n",
    "with open('ai.ja.txt.parsed','r') as lines:\n",
    "    \n",
    "    for line in lines:\n",
    "\n",
    "        # 各行について文節情報のパターンを確認\n",
    "        matchs = re.findall(pattern,line)\n",
    "\n",
    "        # 最終行の場合は終了\n",
    "        if line == 'EOS':\n",
    "            break\n",
    "\n",
    "        # 文節情報を保持する場合\n",
    "        elif matchs:\n",
    "\n",
    "            # 文節情報を取り出し表示する\n",
    "            match_list = line.split(' ')\n",
    "\n",
    "            # 今回は冒頭の文章のみ表示するのでidが1まで行ったら終了\n",
    "            if match_list[1] == '1':\n",
    "                break\n",
    "\n",
    "            print(match_list)\n",
    "        \n",
    "        # 形態素情報の場合\n",
    "        else:\n",
    "\n",
    "            line = line.split('\\t')\n",
    "\n",
    "            # リスト化して、形態素と品詞などの情報を持つ部分に分ける\n",
    "            surface,sentences = line[0],line[1]\n",
    "\n",
    "            # さらにカンマで分割して基本形や品詞などに分ける\n",
    "            sentence = sentences.split(',')\n",
    "            (base,pos,pos1) = sentence[6],sentence[0],sentence[1]\n",
    "\n",
    "            # Morphクラスのインスタンスを生成し表示\n",
    "            m = Morph(surface=surface,base=base,pos=pos,pos1=pos1)\n",
    "            print(m.return_parsed())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "41. 係り受け解析結果の読み込み（文節・係り受け）\n",
    "\n",
    "40に加えて，文節を表すクラスChunkを実装せよ．このクラスは形態素（Morphオブジェクト）のリスト（morphs），係り先文節インデックス番号（dst），係り元文節インデックス番号のリスト（srcs）をメンバ変数に持つこととする．さらに，入力テキストの係り受け解析結果を読み込み，１文をChunkオブジェクトのリストとして表現し，冒頭の説明文の文節の文字列と係り先を表示せよ．本章の残りの問題では，ここで作ったプログラムを活用せよ．"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "インデックス：2937\n",
      "文節：発言し\n",
      "係先インデックス：2941\n",
      "\n",
      "インデックス：2938\n",
      "文節：伊勢田は\n",
      "係先インデックス：2941\n",
      "\n",
      "インデックス：2939\n",
      "文節：決着は\n",
      "係先インデックス：2940\n",
      "\n",
      "インデックス：2940\n",
      "文節：つかないでしょうねと\n",
      "係先インデックス：2941\n",
      "\n",
      "インデックス：2941\n",
      "文節：答えている\n",
      "係先インデックス：-1\n",
      "\n"
     ]
    }
   ],
   "source": [
    "class Chunk:\n",
    "    \n",
    "    # イニシャライザを上記の通り設定\n",
    "    def __init__(self, morphs, dst, srcs, chunk_id):\n",
    "\n",
    "        self.morphs = morphs\n",
    "        self.dst = dst\n",
    "        self.srcs = srcs\n",
    "\n",
    "        # 自身が持つid\n",
    "        self.chunk_id = chunk_id\n",
    "\n",
    "        # 43 文節における名詞・動詞の有無フラグ\n",
    "        self.has_noun = False\n",
    "        self.has_verb = False\n",
    "    \n",
    "    # 文節、係先のid、自身が持つidを返す\n",
    "    def return_string_dependency(self):\n",
    "\n",
    "        surface = []\n",
    "\n",
    "        for morph in self.morphs:\n",
    "\n",
    "            surface.append(morph.surface)\n",
    "\n",
    "        return \"\".join(surface),self.dst,self.chunk_id\n",
    "    \n",
    "    # 係元のidリスト、自身が持つidを返す\n",
    "    def return_string_sources(self):\n",
    "\n",
    "        return self.srcs,self.chunk_id\n",
    "\n",
    "# 文節ごとに係先の情報や自身のidを保持しているので、パターン化\n",
    "pattern = r'^\\* \\d+ -?[0-9]+[A-Z] \\d+/\\d+ -?\\d+\\.\\d+$'\n",
    "\n",
    "morphs = [] # 文節の全形態素Morphクラスを保持\n",
    "chunks = [] # 全文章の文節Chunkクラスを保持\n",
    "dst = None # 係先インデックス\n",
    "srcs = [] # 係元インデックス(複数ある)\n",
    "chunk_id = -1 # 自身のid\n",
    "has_noun = False # 名詞の有無\n",
    "has_verb = False # 動詞の有無\n",
    "\n",
    "with open('ai.ja.txt.parsed','r') as lines:\n",
    "\n",
    "    for line in lines:\n",
    "\n",
    "        # 各行について文節情報のパターンを確認\n",
    "        matchs = re.findall(pattern,line)\n",
    "\n",
    "        # EOSに当たった場合\n",
    "        if line == 'EOS':\n",
    "\n",
    "            # 得られた文節列をChunkインスタンスとしてchunksに追加 - (*)\n",
    "            c = Chunk(morphs,dst,srcs,chunk_id)\n",
    "            chunks.append(c)\n",
    "\n",
    "            # 43 文節における名詞・動詞の有無\n",
    "            # 名刺・動詞を持っている場合は適宜内部のフラグを書き換える\n",
    "            if has_noun:\n",
    "                c.has_noun = True\n",
    "            elif has_verb:\n",
    "                c.has_verb = True\n",
    "            break\n",
    "        \n",
    "        # 文節情報を保持する場合\n",
    "        elif matchs:\n",
    "\n",
    "            match_list = line.split(' ')\n",
    "\n",
    "            # 一番最初はやらない\n",
    "            if chunk_id < 0:\n",
    "                chunk_id = 0\n",
    "            \n",
    "            # 2回目以降\n",
    "            else:\n",
    "\n",
    "                # (*) と同じ\n",
    "                c = Chunk(morphs,dst,srcs,chunk_id)\n",
    "                chunks.append(c)\n",
    "                if has_noun:\n",
    "                    c.has_noun = True\n",
    "                elif has_verb:\n",
    "                    c.has_verb = True\n",
    "            \n",
    "            # 一度文節情報をリセットする\n",
    "            # ただし自身のidと係先のidは次の処理のため保持しておく\n",
    "            morphs = []\n",
    "            dst = int(match_list[2].replace(\"D\", \"\"))\n",
    "            chunk_id = int(match_list[1])\n",
    "            srcs=[]\n",
    "            has_noun = False\n",
    "            has_verb = False\n",
    "        \n",
    "        # 形態素情報の場合\n",
    "        else:\n",
    "\n",
    "            # 40 で形態素情報を得られた場合と同じく形態素列を作成\n",
    "            line = line.split('\\t')\n",
    "            (surface,sentences) = line[0],line[1]\n",
    "            sentence = sentences.split(',')\n",
    "            (base,pos,pos1) = sentence[6],sentence[0],sentence[1]\n",
    "\n",
    "            # 42 記号削除\n",
    "            # 記号があれば形態素を空の状態にして\n",
    "            # それ以外は得られた形態素情報をそれぞれ格納する\n",
    "            if pos == '記号':\n",
    "                morphs.append(Morph('',base,pos,pos1))\n",
    "            else:\n",
    "                morphs.append(Morph(surface,base,pos,pos1))   \n",
    "            \n",
    "            # 43 文節における名詞・動詞の有無\n",
    "            # 動詞・名詞を持っていれば変数を更新しておく\n",
    "            if pos == '名詞':\n",
    "                has_noun = True\n",
    "            elif pos == '動詞':\n",
    "                has_verb = True\n",
    "                     \n",
    "# 後ろの5行について、インデックス、文節、係先インデックスを表示\n",
    "for chunk in chunks[-5:]:\n",
    "    (sentence,dst,id) = chunk.return_string_dependency()\n",
    "    print(f\"インデックス：{id}\\n文節：{sentence}\\n係先インデックス：{dst}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "42. 係り元と係り先の文節の表示\n",
    "\n",
    "係り元の文節と係り先の文節のテキストをタブ区切り形式ですべて抽出せよ．ただし，句読点などの記号は出力しないようにせよ．"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[]\t語\n",
      "[]\t語\n",
      "[]\tエーアイとは\n",
      "['AI']\t語\n",
      "[]\tという\n",
      "['計算']\t道具を\n",
      "[]\t道具を\n",
      "[]\tという\n",
      "['コンピュータ']\t道具を\n",
      "['という', '概念と', 'という']\t用いて\n",
      "['道具を']\t研究する\n",
      "[]\t研究する\n",
      "['用いて', '知能を']\t計算機科学\n",
      "['研究する']\tの\n"
     ]
    }
   ],
   "source": [
    "# 係元インデックスリストを作成\n",
    "for chunk in range(len(chunks)):\n",
    "\n",
    "    # 各文節クラスについて係先のインデックスを取得し、係先の文節クラスの係元インデックスを追加\n",
    "    (sentence,dst,id) = chunks[chunk].return_string_dependency()\n",
    "    chunks[dst].srcs.append(id)\n",
    "\n",
    "# 一部表示\n",
    "for chunk in range(len(chunks)//200):\n",
    "\n",
    "    source = []\n",
    "\n",
    "    # 係元インデックスリストを得る\n",
    "    (srcs, id) = chunks[chunk].return_string_sources()\n",
    "\n",
    "    for srcs_index in srcs:\n",
    "        if srcs_index < 0:\n",
    "            continue\n",
    "\n",
    "        # 係元インデックスの文節を全て受け取りsourceに追加\n",
    "        (mor, _, _)=chunks[srcs_index].return_string_dependency()\n",
    "        source.append(mor)\n",
    "    \n",
    "    # 各文節の係先インデックスの文節も受け取る\n",
    "    (main, dst, _)=chunks[chunk].return_string_dependency()\n",
    "    dst_mor = None\n",
    "    if dst > 0:\n",
    "        (dst_mor, _, _)=chunks[dst].return_string_dependency()\n",
    "    \n",
    "    # 係元文節リストと係先文節をタブで区切って表示\n",
    "    print(f\"{source}\\t{dst_mor}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "43. 名詞を含む文節が動詞を含む文節に係るものを抽出\n",
    "\n",
    "名詞を含む文節が，動詞を含む文節に係るとき，これらをタブ区切り形式で抽出せよ．ただし，句読点などの記号は出力しないようにせよ．"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "道具を\t用いて\n",
      "一分野を\t指す\n",
      "知的行動を\t代わって\n",
      "人間に\t代わって\n",
      "コンピューターに\t行わせる\n",
      "研究分野とも\tされる\n",
      "解説で\t述べている\n",
      "佐藤理史は\t述べている\n",
      "次のように\t述べている\n",
      "技術ソフトウェアコンピュータシステム\tある\n",
      "応用例は\tある\n"
     ]
    }
   ],
   "source": [
    "for chunk in range(len(chunks)//50):\n",
    "\n",
    "    # 各文節クラスについて、文節nと係先インデックスを得る\n",
    "    (mor_n, dst, _) = chunks[chunk].return_string_dependency()\n",
    "\n",
    "    # 係先の文節vも得る\n",
    "    (mor_v, _, _) = chunks[dst].return_string_dependency()\n",
    "\n",
    "    # 文節nが名詞を含み、文節vが動詞を含むとき、それらをタブ区切りで表示\n",
    "    if dst > 0 and chunks[chunk].has_noun and chunks[dst].has_verb:\n",
    "        print(f\"{mor_n}\\t{mor_v}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "44. 係り受け木の可視化\n",
    "\n",
    "与えられた文の係り受け木を有向グラフとして可視化せよ．可視化には，[Graphviz](http://www.graphviz.org/)等を用いるとよい．\n",
    "(今回用いたインターフェースは[これ](https://graphviz.readthedocs.io/en/stable/manual.html))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/svg+xml": [
       "<?xml version=\"1.0\" encoding=\"UTF-8\" standalone=\"no\"?>\n",
       "<!DOCTYPE svg PUBLIC \"-//W3C//DTD SVG 1.1//EN\"\n",
       " \"http://www.w3.org/Graphics/SVG/1.1/DTD/svg11.dtd\">\n",
       "<!-- Generated by graphviz version 9.0.0 (20230911.1827)\n",
       " -->\n",
       "<!-- Pages: 1 -->\n",
       "<svg width=\"466pt\" height=\"260pt\"\n",
       " viewBox=\"0.00 0.00 465.76 260.00\" xmlns=\"http://www.w3.org/2000/svg\" xmlns:xlink=\"http://www.w3.org/1999/xlink\">\n",
       "<g id=\"graph0\" class=\"graph\" transform=\"scale(1 1) rotate(0) translate(4 256)\">\n",
       "<polygon fill=\"white\" stroke=\"none\" points=\"-4,4 -4,-256 461.76,-256 461.76,4 -4,4\"/>\n",
       "<!-- ジョンマッカーシーは -->\n",
       "<g id=\"node1\" class=\"node\">\n",
       "<title>ジョンマッカーシーは</title>\n",
       "<ellipse fill=\"none\" stroke=\"black\" cx=\"102.53\" cy=\"-90\" rx=\"102.53\" ry=\"18\"/>\n",
       "<text text-anchor=\"middle\" x=\"102.53\" y=\"-84.2\" font-family=\"Times,serif\" font-size=\"14.00\">ジョンマッカーシーは</text>\n",
       "</g>\n",
       "<!-- 作り出した -->\n",
       "<g id=\"node2\" class=\"node\">\n",
       "<title>作り出した</title>\n",
       "<ellipse fill=\"none\" stroke=\"black\" cx=\"261.53\" cy=\"-18\" rx=\"56.71\" ry=\"18\"/>\n",
       "<text text-anchor=\"middle\" x=\"261.53\" y=\"-12.2\" font-family=\"Times,serif\" font-size=\"14.00\">作り出した</text>\n",
       "</g>\n",
       "<!-- ジョンマッカーシーは&#45;&gt;作り出した -->\n",
       "<g id=\"edge1\" class=\"edge\">\n",
       "<title>ジョンマッカーシーは&#45;&gt;作り出した</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M139.41,-72.76C163.17,-62.3 194.14,-48.67 218.95,-37.75\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"220.03,-41.09 227.78,-33.86 217.21,-34.69 220.03,-41.09\"/>\n",
       "</g>\n",
       "<!-- AIに関する -->\n",
       "<g id=\"node3\" class=\"node\">\n",
       "<title>AIに関する</title>\n",
       "<ellipse fill=\"none\" stroke=\"black\" cx=\"251.53\" cy=\"-234\" rx=\"59.54\" ry=\"18\"/>\n",
       "<text text-anchor=\"middle\" x=\"251.53\" y=\"-228.95\" font-family=\"Times,serif\" font-size=\"14.00\">AIに関する</text>\n",
       "</g>\n",
       "<!-- 最初の -->\n",
       "<g id=\"node4\" class=\"node\">\n",
       "<title>最初の</title>\n",
       "<ellipse fill=\"none\" stroke=\"black\" cx=\"251.53\" cy=\"-162\" rx=\"38.18\" ry=\"18\"/>\n",
       "<text text-anchor=\"middle\" x=\"251.53\" y=\"-156.2\" font-family=\"Times,serif\" font-size=\"14.00\">最初の</text>\n",
       "</g>\n",
       "<!-- AIに関する&#45;&gt;最初の -->\n",
       "<g id=\"edge2\" class=\"edge\">\n",
       "<title>AIに関する&#45;&gt;最初の</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M251.53,-215.7C251.53,-208.41 251.53,-199.73 251.53,-191.54\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"255.03,-191.62 251.53,-181.62 248.03,-191.62 255.03,-191.62\"/>\n",
       "</g>\n",
       "<!-- 会議で -->\n",
       "<g id=\"node5\" class=\"node\">\n",
       "<title>会議で</title>\n",
       "<ellipse fill=\"none\" stroke=\"black\" cx=\"261.53\" cy=\"-90\" rx=\"38.18\" ry=\"18\"/>\n",
       "<text text-anchor=\"middle\" x=\"261.53\" y=\"-84.2\" font-family=\"Times,serif\" font-size=\"14.00\">会議で</text>\n",
       "</g>\n",
       "<!-- 最初の&#45;&gt;会議で -->\n",
       "<g id=\"edge3\" class=\"edge\">\n",
       "<title>最初の&#45;&gt;会議で</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M254,-143.7C255.05,-136.32 256.31,-127.52 257.49,-119.25\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"260.92,-120 258.87,-109.6 253.99,-119.01 260.92,-120\"/>\n",
       "</g>\n",
       "<!-- 会議で&#45;&gt;作り出した -->\n",
       "<g id=\"edge4\" class=\"edge\">\n",
       "<title>会議で&#45;&gt;作り出した</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M261.53,-71.7C261.53,-64.41 261.53,-55.73 261.53,-47.54\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"265.03,-47.62 261.53,-37.62 258.03,-47.62 265.03,-47.62\"/>\n",
       "</g>\n",
       "<!-- 人工知能という -->\n",
       "<g id=\"node6\" class=\"node\">\n",
       "<title>人工知能という</title>\n",
       "<ellipse fill=\"none\" stroke=\"black\" cx=\"382.53\" cy=\"-162\" rx=\"75.23\" ry=\"18\"/>\n",
       "<text text-anchor=\"middle\" x=\"382.53\" y=\"-156.2\" font-family=\"Times,serif\" font-size=\"14.00\">人工知能という</text>\n",
       "</g>\n",
       "<!-- 用語を -->\n",
       "<g id=\"node7\" class=\"node\">\n",
       "<title>用語を</title>\n",
       "<ellipse fill=\"none\" stroke=\"black\" cx=\"373.53\" cy=\"-90\" rx=\"38.18\" ry=\"18\"/>\n",
       "<text text-anchor=\"middle\" x=\"373.53\" y=\"-84.2\" font-family=\"Times,serif\" font-size=\"14.00\">用語を</text>\n",
       "</g>\n",
       "<!-- 人工知能という&#45;&gt;用語を -->\n",
       "<g id=\"edge5\" class=\"edge\">\n",
       "<title>人工知能という&#45;&gt;用語を</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M380.3,-143.7C379.36,-136.32 378.22,-127.52 377.16,-119.25\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"380.67,-119.08 375.92,-109.61 373.72,-119.97 380.67,-119.08\"/>\n",
       "</g>\n",
       "<!-- 用語を&#45;&gt;作り出した -->\n",
       "<g id=\"edge6\" class=\"edge\">\n",
       "<title>用語を&#45;&gt;作り出した</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M351.13,-75C335.3,-65.11 313.77,-51.65 295.79,-40.41\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"297.84,-37.57 287.5,-35.24 294.13,-43.5 297.84,-37.57\"/>\n",
       "</g>\n",
       "</g>\n",
       "</svg>\n"
      ],
      "text/plain": [
       "<graphviz.graphs.Digraph at 0x107ad24f0>"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# これだと枝が重複するかも\n",
    "# 固有IDあった方が良い\n",
    "branchs = []\n",
    "d = graphviz.Digraph()\n",
    "\n",
    "# 「ジョン・マッカーシーはAIに関する最初の会議で人工知能という用語を作り出した。」\n",
    "for chunk in range(357,363):\n",
    "\n",
    "    # 原文節をtail、係先文節をheadとして保持\n",
    "    (tail, dst, _) = chunks[chunk].return_string_dependency()\n",
    "    (head, _, _) = chunks[dst].return_string_dependency()\n",
    "\n",
    "    # 枝として保存していく\n",
    "    if dst > 0:\n",
    "        d.edge(tail,head)\n",
    "\n",
    "# 一文の係り受け木を表示\n",
    "d"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "45. 動詞の格パターンの抽出\n",
    "\n",
    "今回用いている文章をコーパスと見なし，日本語の述語が取りうる格を調査したい． 動詞を述語，動詞に係っている文節の助詞を格と考え，述語と格をタブ区切り形式で出力せよ． ただし，出力は以下の仕様を満たすようにせよ．\n",
    "\n",
    "- 動詞を含む文節において，最左の動詞の基本形を述語とする\n",
    "- 述語に係る助詞を格とする\n",
    "- 述語に係る助詞（文節）が複数あるときは，すべての助詞をスペース区切りで辞書順に並べる\n",
    "\n",
    "「ジョン・マッカーシーはAIに関する最初の会議で人工知能という用語を作り出した。」という例文を考える． この文は「作り出す」という１つの動詞を含み，「作り出す」に係る文節は「ジョン・マッカーシーは」，「会議で」，「用語を」であると解析された場合は，次のような出力になるはずである．\n",
    "\n",
    "```zsh\n",
    "作り出す\tで は を\n",
    "```\n",
    "\n",
    "このプログラムの出力をファイルに保存し，以下の事項をUNIXコマンドを用いて確認せよ．\n",
    "\n",
    "- コーパス中で頻出する述語と格パターンの組み合わせ\n",
    "- 「行う」「なる」「与える」という動詞の格パターン（コーパス中で出現頻度の高い順に並べよ）\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 書き込みモードで開く\n",
    "f = open('verb_kaku.txt','w')\n",
    "\n",
    "# 動詞に助詞がかかる場合のみ取り出す\n",
    "for chunk in range(len(chunks)):\n",
    "\n",
    "    # 文節が動詞を持つ場合のみ実行\n",
    "    if chunks[chunk].has_verb:\n",
    "\n",
    "        verb = None\n",
    "        morphs = chunks[chunk].morphs\n",
    "\n",
    "        for morph in morphs:\n",
    "\n",
    "            # 各形態素の中から動詞のものを探して基本形を取り出す\n",
    "            (_, base, pos, _) = morph.return_parsed()\n",
    "            if pos == '動詞':\n",
    "                verb = base\n",
    "                break\n",
    "        \n",
    "        kakus = []\n",
    "        (srcs, id) = chunks[chunk].return_string_sources()\n",
    "\n",
    "        for srcs_index in srcs:\n",
    "\n",
    "            if srcs_index < 0:\n",
    "                continue\n",
    "\n",
    "            kaku = ''\n",
    "            morphs=chunks[srcs_index].morphs\n",
    "\n",
    "            for morph in morphs:\n",
    "\n",
    "                # 各形態素の中から助詞のものを探して基本形を取り出す\n",
    "                (_, base, pos, _) = morph.return_parsed()\n",
    "                if pos == '助詞':\n",
    "                    kaku = base\n",
    "            \n",
    "            kakus.append(kaku)\n",
    "        \n",
    "        # print(verb+'\\t',*kakus,sep=' ')\n",
    "            \n",
    "        # まず動詞を書き出しタブで区切り、格をスペースで区切って書き出す\n",
    "        f.write(verb+'\\t'+' '.join(sorted(kakus))+'\\n')\n",
    "\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- コーパス中で頻出する述語と格パターンの組み合わせ\n",
    "\n",
    "```zsh\n",
    "\n",
    "% sort verb_kaku.txt | uniq -c | sort -n -r | head\n",
    "  11 よる       に\n",
    "   7 れる       と\n",
    "   6 する       と\n",
    "   5 用いる     を\n",
    "   5 基づく     に\n",
    "   4 行う       を\n",
    "   4 ある       が\n",
    "   3 関わる     も\n",
    "   3 向ける     に\n",
    "   3 受ける     を\n",
    "\n",
    "```\n",
    "\n",
    "- 「行う」「なる」「与える」という動詞の格パターン（コーパス中で出現頻度の高い順に並べよ）\n",
    "\n",
    "```zsh\n",
    "\n",
    "% grep \"^行う\\t\" verb_kaku.txt | sort | uniq -c | sort -n -r | head\n",
    "   4 行う       を\n",
    "   1 行う       に を を\n",
    "   1 行う       で に を\n",
    "   1 行う        は を\n",
    "   1 行う        で を\n",
    "\n",
    "% grep \"^なる\\t\" verb_kaku.txt | sort | uniq -c | sort -n -r | head\n",
    "   2 なる       が と\n",
    "   1 なる       が にとって は\n",
    "   1 なる       と など\n",
    "   1 なる       が て と\n",
    "   1 なる       は を\n",
    "   1 なる       に は\n",
    "   1 なる       が に\n",
    "   1 なる       に\n",
    "   1 なる       と\n",
    "   1 なる        が で と に は は\n",
    "\n",
    "% grep \"^与える\\t\" verb_kaku.txt | sort | uniq -c | sort -n -r | head\n",
    "   1 与える     に は を\n",
    "   1 与える     が に\n",
    "\n",
    "```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "46. 動詞の格フレーム情報の抽出\n",
    "\n",
    "45のプログラムを改変し，述語と格パターンに続けて項（述語に係っている文節そのもの）をタブ区切り形式で出力せよ．45の仕様に加えて，以下の仕様を満たすようにせよ．\n",
    "\n",
    "- 項は述語に係っている文節の単語列とする（末尾の助詞を取り除く必要はない）\n",
    "- 述語に係る文節が複数あるときは，助詞と同一の基準・順序でスペース区切りで並べる\n",
    "\n",
    "「ジョン・マッカーシーはAIに関する最初の会議で人工知能という用語を作り出した。」という例文を考える． この文は「作り出す」という１つの動詞を含み，「作り出す」に係る文節は「ジョン・マッカーシーは」，「会議で」，「用語を」であると解析された場合は，次のような出力になるはずである．\n",
    "\n",
    "```zsh\n",
    "作り出す\tで は を\t会議で ジョンマッカーシーは 用語を\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "f = open('verb_kaku_kou.txt','w')\n",
    "\n",
    "# 45 に項を追加\n",
    "for chunk in range(len(chunks)):\n",
    "\n",
    "    # 文節が動詞を持つ場合のみ実行\n",
    "    if chunks[chunk].has_verb:\n",
    "\n",
    "        verb = None\n",
    "        morphs = chunks[chunk].morphs\n",
    "\n",
    "        for morph in morphs:\n",
    "\n",
    "            # 各形態素の中から動詞のものを探して基本形を取り出す\n",
    "            (_, base, pos, _) = morph.return_parsed()\n",
    "            if pos == '動詞':\n",
    "                verb = base\n",
    "                break\n",
    "        \n",
    "        kakus, kous = [], []\n",
    "        (srcs, id) = chunks[chunk].return_string_sources()\n",
    "\n",
    "        for srcs_index in srcs:\n",
    "\n",
    "            if srcs_index < 0:\n",
    "                continue\n",
    "\n",
    "            kaku, kou = '', ''\n",
    "            morphs = chunks[srcs_index].morphs\n",
    "            (mor, _, _) = chunks[srcs_index].return_string_dependency()\n",
    "            \n",
    "            for morph in morphs:\n",
    "\n",
    "                # 各形態素の中から助詞のものを探して基本形を取り出す\n",
    "                # 助詞だった場合はその文節ごと取り出す\n",
    "                (_, base, pos, _) = morph.return_parsed()\n",
    "                if pos == '助詞':\n",
    "                    kaku = base\n",
    "                    kou = mor\n",
    "\n",
    "            # 助詞と文節を配列で格納\n",
    "            kakus.append([kaku, kou])\n",
    "\n",
    "        # 助詞に対してkakusをソートし、動詞、格、項の順にタブ区切りで書き出す\n",
    "        kakus = sorted(kakus, key=lambda x: x[0])\n",
    "        f.write(verb+'\\t'+' '.join([x[0] for x in kakus])+'\\t'+' '.join([x[1] for x in kakus])+'\\n')\n",
    "\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 出力結果における先頭10行\n",
    "\n",
    "```txt\n",
    "用いる\tを\t道具を\n",
    "指す\tを\t一分野を\n",
    "代わる\tに を\t人間に 知的行動を\n",
    "せる\tて に\t代わって コンピューターに\n",
    "れる\tも\t研究分野とも\n",
    "いる\tで に は\t解説で 次のように 佐藤理史は\n",
    "ある\t が は\t 画像認識等が 応用例は\n",
    "用いる\tを\t記号処理を\n",
    "する\tと を\t主体と 記述を\n",
    "いる\t でも は\t 意味あいでも 現在では\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "47. 機能動詞構文のマイニング\n",
    "\n",
    "動詞のヲ格にサ変接続名詞が入っている場合のみに着目したい．46のプログラムを以下の仕様を満たすように改変せよ．\n",
    "\n",
    "- 「サ変接続名詞+を（助詞）」で構成される文節が動詞に係る場合のみを対象とする\n",
    "- 述語は「サ変接続名詞+を+動詞の基本形」とし，文節中に複数の動詞があるときは，最左の動詞を用いる\n",
    "- 述語に係る助詞（文節）が複数あるときは，すべての助詞をスペース区切りで辞書順に並べる\n",
    "- 述語に係る文節が複数ある場合は，すべての項をスペース区切りで並べる（助詞の並び順と揃えよ）\n",
    "\n",
    "例えば「また、自らの経験を元に学習を行う強化学習という手法もある。」という文から，以下の出力が得られるはずである．\n",
    "\n",
    "```\n",
    "学習を行う\tに を\t元に 経験を\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "f = open('sahen_wo_verb.txt','w')\n",
    "\n",
    "# 動詞のヲ格にサ変接続名詞が入っている場合のみに着目\n",
    "for chunk in range(len(chunks)):\n",
    "\n",
    "    # # 文節が動詞を持つ場合のみ実行\n",
    "    if chunks[chunk].has_verb:\n",
    "\n",
    "        verb = None\n",
    "        morphs = chunks[chunk].morphs\n",
    "\n",
    "        for morph in morphs:\n",
    "\n",
    "            # 各形態素の中から動詞のものを探して基本形を取り出す\n",
    "            (_, base, pos, _) = morph.return_parsed()\n",
    "            if pos == '動詞':\n",
    "                verb = base\n",
    "                break\n",
    "\n",
    "        kakus, kous = [], []\n",
    "        sahen_wo = ''\n",
    "        sahen_wo_verb = None\n",
    "        (srcs, id) = chunks[chunk].return_string_sources()\n",
    "\n",
    "        for srcs_index in srcs:\n",
    "            \n",
    "            if srcs_index < 0:\n",
    "                continue\n",
    "\n",
    "            kaku, kou = '', ''\n",
    "            morphs = chunks[srcs_index].morphs\n",
    "            (mor, _, _) = chunks[srcs_index].return_string_dependency()\n",
    "\n",
    "            if len(morphs) > 1 :\n",
    "\n",
    "                # 形態素列が複数ある場合のみ\n",
    "                if morphs[0].pos1 == 'サ変接続' and morphs[1].surface == 'を' and morphs[1].pos == '助詞':\n",
    "\n",
    "                    # サ変接続名詞+を(助詞) のときのみ満たしているので、sahen_woに代入\n",
    "                    sahen_wo = morphs[0].surface + morphs[1].surface\n",
    "\n",
    "                    # 動詞も接続したsahen_wo_verbも別に作っておく\n",
    "                    sahen_wo_verb = morphs[0].surface + morphs[1].surface + verb\n",
    "\n",
    "            for morph in morphs:\n",
    "\n",
    "                # 各形態素の中から助詞のものを探して基本形を取り出す\n",
    "                # 助詞だった場合はその文節ごと取り出す\n",
    "                (_, base, pos, _) = morph.return_parsed()\n",
    "                if pos == '助詞':\n",
    "                    kaku = base\n",
    "                    kou = mor\n",
    "            \n",
    "            # 助詞と文節を配列で格納\n",
    "            kakus.append([kaku, kou])\n",
    "\n",
    "        for i in range(len(kakus)):\n",
    "\n",
    "            # 「サ変接続名詞 + を」 の形であった場合は述語として表示されるためいらない\n",
    "            if kakus[i][1] == sahen_wo:\n",
    "                kakus.pop(i)\n",
    "                break\n",
    "        \n",
    "        # 助詞に対してkakusをソート\n",
    "        # 「サ変接続名詞+を+動詞の基本形」の場合のみ、述語、格、項の順にタブ区切りで書き出す\n",
    "        kakus = sorted(kakus, key=lambda x: x[0])\n",
    "        if sahen_wo_verb:\n",
    "            f.write(sahen_wo_verb+'\\t'+' '.join([x[0] for x in kakus])+'\\t'+' '.join([x[1] for x in kakus])+'\\n')\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 出力結果\n",
    "\n",
    "```txt\n",
    "記述をする\tと\t主体と\n",
    "注目を集める\t   が\t   サポートベクターマシンが\n",
    "学習を行う\tに を\t元に 経験を\n",
    "流行を超える\t\t\n",
    "学習を繰り返す\t\t\n",
    "進化を見せる\t て て において は\t 活躍している 加えて 生成技術において 敵対的生成ネットワークは\n",
    "開発を行う\t は\t エイダ・ラブレスは\n",
    "処理を行う\t\t\n",
    "研究を進める\tて\t費やして\n",
    "注目を集める\t  から は\t  ことから ファジィは\n",
    "成功を受ける\t\t\n",
    "進歩を担う\t\t\n",
    "研究を続ける\tが て\tジェフ・ホーキンスが 向けて\n",
    "注目を集める\tに\t急速に\n",
    "普及を受ける\t\t\n",
    "投資を行う\tで に\t民間企業主導で 全世界的に\n",
    "探索を行う\t で\t 無報酬で\n",
    "研究を行う\t て\t 始めており\n",
    "実験をする\t\t\n",
    "投資をする\t に は\t 2022年までに 韓国は\n",
    "反乱を起こす\tて に対して\t於いて 人間に対して\n",
    "弾圧を併せ持つ\t\t\n",
    "監視を行う\t に まで\t 人工知能に 歩行者まで\n",
    "差別を認める\t\t\n",
    "展開を変える\t\t\n",
    "判断を介す\tから\t観点から\n",
    "禁止を求める\t が は\t ヒューマン・ライツ・ウォッチが 4月には\n",
    "運用をめぐる\t\t\n",
    "試験を行う\t\t\n",
    "追及を受ける\t て で と とともに は\t 暴露されており 整合性で 拒否すると とともに 公聴会では\n",
    "話をする\t は\t 哲学者は\n",
    "議論を行う\t  まで\t  これまで\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "48. 名詞から根へのパスの抽出\n",
    "\n",
    "文中のすべての名詞を含む文節に対し，その文節から構文木の根に至るパスを抽出せよ． ただし，構文木上のパスは以下の仕様を満たすものとする．\n",
    "\n",
    "- 各文節は（表層形の）形態素列で表現する\n",
    "- パスの開始文節から終了文節に至るまで，各文節の表現を” -> “で連結する\n",
    "\n",
    "「ジョン・マッカーシーはAIに関する最初の会議で人工知能という用語を作り出した。」という例文を考える． CaboChaを係り受け解析に用いた場合，次のような出力が得られると思われる．\n",
    "\n",
    "```zsh\n",
    "ジョンマッカーシーは -> 作り出した\n",
    "AIに関する -> 最初の -> 会議で -> 作り出した\n",
    "最初の -> 会議で -> 作り出した\n",
    "会議で -> 作り出した\n",
    "人工知能という -> 用語を -> 作り出した\n",
    "用語を -> 作り出した\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ジョンマッカーシーは -> 作り出した\n",
      "AIに関する -> 最初の -> 会議で -> 作り出した\n",
      "最初の -> 会議で -> 作り出した\n",
      "会議で -> 作り出した\n",
      "人工知能という -> 用語を -> 作り出した\n",
      "用語を -> 作り出した\n"
     ]
    }
   ],
   "source": [
    "# 文節の端と端を決める\n",
    "start, end = 357, 363\n",
    "\n",
    "for chunk in range(start,end):\n",
    "\n",
    "    dst, col = chunk, []\n",
    "\n",
    "    # 係先が一文に収まる間は矢印で結合していく\n",
    "    # これを一文のすべての文節に対して継続する\n",
    "    while start <= dst and dst <= end:\n",
    "        \n",
    "        sur, dst, _ =  chunks[dst].return_string_dependency()\n",
    "        col.append(sur)\n",
    "    \n",
    "    print(\" -> \".join(col))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "49. 名詞間の係り受けパスの抽出Permalink\n",
    "\n",
    "文中のすべての名詞句のペアを結ぶ最短係り受けパスを抽出せよ．ただし，名詞句ペアの文節番号がi\n",
    "とj（i < j）のとき，係り受けパスは以下の仕様を満たすものとする．\n",
    "\n",
    "- 問題48と同様に，パスは開始文節から終了文節に至るまでの各文節の表現（表層形の形態素列）を” -> “で連結して表現する\n",
    "- 文節iとjに含まれる名詞句はそれぞれ，XとYに置換する\n",
    "\n",
    "また，係り受けパスの形状は，以下の2通りが考えられる．\n",
    "\n",
    "- 文節iから構文木の根に至る経路上に文節jが存在する場合: 文節iから文節jのパスを表示\n",
    "- 上記以外で，文節iと文節jから構文木の根に至る経路上で共通の文節kで交わる場合: 文節iから文節kに至る直前のパスと文節jから文節kに至る直前までのパス，文節kの内容を” | “で連結して表示\n",
    "\n",
    "「ジョン・マッカーシーはAIに関する最初の会議で人工知能という用語を作り出した。」という例文を考える． CaboChaを係り受け解析に用いた場合，次のような出力が得られると思われる．\n",
    "\n",
    "```zsh\n",
    "Xは | Yに関する -> 最初の -> 会議で | 作り出した\n",
    "Xは | Yの -> 会議で | 作り出した\n",
    "Xは | Yで | 作り出した\n",
    "Xは | Yという -> 用語を | 作り出した\n",
    "Xは | Yを | 作り出した\n",
    "Xに関する -> Yの\n",
    "Xに関する -> 最初の -> Yで\n",
    "Xに関する -> 最初の -> 会議で | Yという -> 用語を | 作り出した\n",
    "Xに関する -> 最初の -> 会議で | Yを | 作り出した\n",
    "Xの -> Yで\n",
    "Xの -> 会議で | Yという -> 用語を | 作り出した\n",
    "Xの -> 会議で | Yを | 作り出した\n",
    "Xで | Yという -> 用語を | 作り出した\n",
    "Xで | Yを | 作り出した\n",
    "Xという -> Yを\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Xは | Yに関する -> 最初の -> 会議で | 作り出した\n",
      "Xは | Yの -> 会議で | 作り出した\n",
      "Xは | Yで | 作り出した\n",
      "Xは | Yという -> 用語を | 作り出した\n",
      "Xは | Yを | 作り出した\n",
      "Xに関する -> Yの\n",
      "Xに関する -> 最初の -> Yで\n",
      "Xに関する -> 最初の -> 会議で | Yという -> 用語を | 作り出した\n",
      "Xに関する -> 最初の -> 会議で | Yを | 作り出した\n",
      "Xの -> Yで\n",
      "Xの -> 会議で | Yという -> 用語を | 作り出した\n",
      "Xの -> 会議で | Yを | 作り出した\n",
      "Xで | Yという -> 用語を | 作り出した\n",
      "Xで | Yを | 作り出した\n",
      "Xという -> Yを\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "- 文節iに含まれる名詞句はXに置換してnew_iとして返す\n",
    "- 文節jに含まれる名詞句はXに置換してnew_jとして返す\n",
    "- 二つのルートリストの重複を削除したものをrootsとして返す\n",
    "'''\n",
    "def return_path(root_i,root_j):\n",
    "\n",
    "    new_i, new_j = [],[]\n",
    "\n",
    "    # まず文節iのルートについて\n",
    "    for i,clause in enumerate(root_i):\n",
    "\n",
    "        bunnsetu = ''\n",
    "        mor, _, id = clause[1].return_string_dependency()\n",
    "\n",
    "        # 先頭の名刺を含む文節について\n",
    "        if i == 0 :\n",
    "\n",
    "            for morph in clause[1].morphs:\n",
    "\n",
    "                # 形態素が名詞の場合はXに置換する\n",
    "                # 名詞が複数ある場合はまとめてXとする\n",
    "                if morph.pos == '名詞':\n",
    "\n",
    "                    if 'X' in bunnsetu:\n",
    "                        continue\n",
    "                    bunnsetu += 'X'\n",
    "                \n",
    "                # それ以外はそのまま形態素を出力\n",
    "                else:\n",
    "                    bunnsetu += morph.surface\n",
    "        \n",
    "        # 名詞を含まなければ形態素列をそのまま文節として出力\n",
    "        else:\n",
    "            bunnsetu += mor\n",
    "\n",
    "        # 置換したものを新たなルートとして追加\n",
    "        new_i.append((id,bunnsetu))\n",
    "\n",
    "    j_id = int()\n",
    "    include_Y = tuple()\n",
    "\n",
    "    # 文節jのルートについても同じように\n",
    "    for j,clause in enumerate(root_j):\n",
    "\n",
    "        bunnsetu = ''\n",
    "        mor, _, id = clause[1].return_string_dependency()\n",
    "\n",
    "        if j == 0 :\n",
    "\n",
    "            for morph in clause[1].morphs:\n",
    "\n",
    "                if morph.pos == '名詞':\n",
    "\n",
    "                    if 'Y' in bunnsetu:\n",
    "                        continue\n",
    "                    bunnsetu += 'Y'\n",
    "                    # Yを含む要素番号を保持しておく\n",
    "                    j_id = id\n",
    "\n",
    "                else:\n",
    "                    bunnsetu += morph.surface\n",
    "                \n",
    "                # 文節とidを結んだタプルを保持しておく\n",
    "                include_Y = (id,bunnsetu)\n",
    "\n",
    "        else:\n",
    "            bunnsetu += mor\n",
    "\n",
    "        new_j.append((id,bunnsetu))\n",
    "    \n",
    "    for idx in range(len(new_i)):\n",
    "\n",
    "        # ルートi,jに同じ文節を含む場合\n",
    "        # ルートjの名詞句しかYに変わっていないので、ルートiも同じ箇所を変更\n",
    "        if new_i[idx][0] == j_id:\n",
    "            new_i[idx] = include_Y\n",
    "            break\n",
    "\n",
    "    # new_i,new_jのセット型を出力し、idについてソートする\n",
    "    roots = sorted(set(new_i+new_j),key=lambda x:x[0])\n",
    "    \n",
    "    # 配列の要素はすべてタプルなので、配列に変更\n",
    "    roots = [list(r) for r in roots]\n",
    "    new_i = [list(i) for i in new_i]\n",
    "    new_j = [list(j) for j in new_j]\n",
    "\n",
    "    return roots, new_i, new_j\n",
    "\n",
    "# 配列lについて要素xを発見した場合は要素番号を、未発見の場合はNoneを返す\n",
    "def research_list(l,x):\n",
    "    return l.index(x) if x in l else None\n",
    "\n",
    "\n",
    "# 文節の端と端を決める\n",
    "start, end = 357, 363\n",
    "\n",
    "# 文節番号i\n",
    "for i in range(start,end-1):\n",
    "\n",
    "    # 文節番号j\n",
    "    for j in range(i+1,end):\n",
    "        \n",
    "        chunks_i, chunks_j = chunks[i], chunks[j]\n",
    "\n",
    "        # どちらも名詞を含んでいる場合のみ実行\n",
    "        if chunks_i.has_noun and chunks_j.has_noun:\n",
    "\n",
    "            root_i, root_j, dst_i, dst_j = [], [], i, j\n",
    "\n",
    "            # それぞれの文節ついてルートを作成\n",
    "            # idと文節を持つタプルをノードとしてリストルートを作成\n",
    "            while start <= dst_i and dst_i <= end:\n",
    "                mor_i = chunks[dst_i]\n",
    "                _, dst_i, id_i =  chunks[dst_i].return_string_dependency()\n",
    "                root_i.append((id_i,mor_i))\n",
    "            while start <= dst_j and dst_j <= end:\n",
    "                mor_j = chunks[dst_j]\n",
    "                _, dst_j, id_j =  chunks[dst_j].return_string_dependency()\n",
    "                root_j.append((id_j,mor_j))\n",
    "        \n",
    "            roots, new_i, new_j  = return_path(root_i,root_j)\n",
    "\n",
    "            for root in roots:\n",
    "\n",
    "                # rootsに入っている要素をnew_i,new_jについて探索\n",
    "                idx = research_list(new_i,root)\n",
    "                jdx = research_list(new_j,root)\n",
    "\n",
    "                # どちらも要素番号が得られない場合\n",
    "                # 交差点がないので矢印パスを文節ノードの要素として追加\n",
    "                # 出力ノード側に追加する\n",
    "                if idx == None:\n",
    "                    new_j[jdx].append(' -> ')\n",
    "                elif jdx == None:\n",
    "                    new_i[idx].append(' -> ')\n",
    "                \n",
    "                # 両方要素番号が得られた場合\n",
    "                else:\n",
    "\n",
    "                    # iの途中に名詞文節jを含む場合はそれ以降を切り捨て\n",
    "                    # それぞれ結合して終了\n",
    "                    if 'Y' in root[1]:\n",
    "                        tail = new_i[:idx]+new_j[:jdx+1]\n",
    "                        break\n",
    "\n",
    "                    # ルートiの文節ノードがパスを含む場合\n",
    "                    if len(new_i[idx-1]) > 2:\n",
    "                        # すでに登録されているパスを' | 'に更新する\n",
    "                        new_i[idx-1][2] = ' | '\n",
    "                    \n",
    "                    # ルートiの文節ノードがパスを含まない場合\n",
    "                    else:\n",
    "                        # 追加する\n",
    "                        new_i[idx-1].append(' | ')\n",
    "\n",
    "                    # ルートjについても行う\n",
    "                    if len(new_j[jdx-1]) > 2:\n",
    "                        new_j[jdx-1][2] = ' | '\n",
    "\n",
    "                    else:\n",
    "                        new_j[jdx-1].append(' | ')\n",
    "                \n",
    "                # 結合箇所は文節が2回出てくるのでnew_i側は削除\n",
    "                tail = new_i[:-1]+new_j\n",
    "            \n",
    "            # 結合箇所はパスを含まないので空白を追加\n",
    "            tail[-1].append('')\n",
    "            col = ''\n",
    "\n",
    "            for t in sorted(tail):\n",
    "\n",
    "                # new_i,new_j両ルートについてidでソート\n",
    "                # 文節とパスをすべて足し合わせる\n",
    "                col += t[1]+t[2]\n",
    "            \n",
    "            print(col)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
